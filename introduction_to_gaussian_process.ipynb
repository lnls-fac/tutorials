{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23a876d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T20:27:26.464939Z",
     "start_time": "2023-02-03T20:27:23.661243Z"
    },
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from functools import partial as _partial\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as scyopt\n",
    "import scipy.stats as scystat\n",
    "from sklearn.datasets import make_regression as _create_data\n",
    "# import gpy\n",
    "import matplotlib.pyplot as mplt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import ipywidgets\n",
    "\n",
    "mplt.rcParams.update({\n",
    "    'font.size':10, 'axes.grid': True, 'grid.alpha': 0.5,\n",
    "    'grid.linestyle': '--', 'grid.linewidth': 1, 'lines.linewidth': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4764f",
   "metadata": {},
   "source": [
    "# Multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2aeeab",
   "metadata": {},
   "source": [
    "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{{#1}}}}}$\n",
    "\n",
    "The multivariate gaussian is the extension of the gaussian distribution to higher dimensions. It can be used to model the joint probability distribution of $N$ random variables $y_i$, which may be thought of as a vector in $\\vect{y}\\in \\mathbb{R}^N$, : \n",
    "\n",
    "$$\\vect{y} \\sim \\mathcal{N}(\\vect{\\mu}, \\vect{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^N|\\vect{\\Sigma}|}}\\exp\\left(-\\frac12(\\vect{y}-\\vect{\\mu})^T\\vect{\\Sigma}^{-1}(\\vect{y}-\\vect{\\mu})\\right)$$\n",
    "\n",
    "This distribution is completely characterized by the vector mean $\\vect{\\mu}\\in\\mathbb{R}^N$ and the covariance matrix $\\vect{\\Sigma}\\in\\mathbb{R}^N\\times\\mathbb{R}^N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb7215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:37:13.974150Z",
     "start_time": "2023-02-03T18:37:13.892710Z"
    },
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = mplt.subplots(1, 1, figsize=(5, 3))\n",
    "ax.set_ylabel('Y1')\n",
    "ax.set_xlabel('Y0')\n",
    "ax.grid(False)\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y1 = np.linspace(-2, 2, 200)\n",
    "y1, y2 = np.meshgrid(y1, y1)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "mean = np.array([0.0, 0.0])\n",
    "pos -= mean\n",
    "\n",
    "surf = ax.pcolormesh(y1, y2, y2)\n",
    "fig.colorbar(surf, label='PDF')\n",
    "\n",
    "@ipywidgets.interact\n",
    "def plot_gauss(correlation=(-0.99, 0.99, 0.01)):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    covi = np.linalg.inv(cov)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    \n",
    "    z = np.exp(-1/2 * np.sum(pos @ covi * pos, axis=-1))\n",
    "    z *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov))\n",
    "\n",
    "    surf.set_array(z)\n",
    "    surf.set_clim([z.min(), z.max()])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d83bc",
   "metadata": {},
   "source": [
    "## Gaussian distributions are closed under important operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c90fe",
   "metadata": {},
   "source": [
    "### Marginalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185869b1",
   "metadata": {},
   "source": [
    "Marginalization is the process of integrating out some variables of the gaussian distribution and only looking at some of them. \n",
    "\n",
    "Consider the join distribution of variables $\\vect{x}$ and $\\vect{y}$:\n",
    "\n",
    "$$\\begin{pmatrix}\\vect{x} \\\\ \\vect{y}\\end{pmatrix} \\sim\n",
    "\\mathcal{N}\\left(\n",
    "    \\begin{pmatrix}\\vect{\\mu_x} \\\\ \\vect{\\mu_y}\\end{pmatrix},\n",
    "    \\begin{pmatrix}\\vect{\\Sigma_{xx}} & \\vect{\\Sigma_{xy}}\\\\ \\vect{\\Sigma_{xy}}^T & \\vect{\\Sigma_{yy}}\\end{pmatrix}\\right) $$\n",
    "\n",
    "If we are interested only on variable $\\vect{x}$ we can integrate out variable $\\vect{y}$:\n",
    "\n",
    "$$\\vect{x} \\sim \\int\\,\\mathrm{d}\\vect{y}P(\\vect{x},\\vect{y}) = \\mathcal{N}\\left(\\vect{\\mu_x}, \\vect{\\Sigma_{xx}}\\right)$$\n",
    "\n",
    "where we notice that not only the result is gaussian, but that all the correlation terms between $\\vect{x}$ and $\\vect{y}$ does not appear on the final result. This means that we could have indefinetly many gaussian-distributed correlated variables, but if we are only interested in a few of them, we don't need to care to all the other correlation terms.\n",
    "\n",
    "We can see this more clearly in the following 2D example, where we notice that varying the correlation does not change the dixtribution of $x$ on the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955b39c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:37:15.081890Z",
     "start_time": "2023-02-03T18:37:14.975678Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('x')\n",
    "ax.set_xlabel('y')\n",
    "ax.grid(False)\n",
    "ay.set_xlabel('x')\n",
    "ay.set_ylabel('PDF(x)')\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y = np.linspace(-4, 4, 200)\n",
    "y1, y2 = np.meshgrid(y, y)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "mean = np.array([0.0, 0.0])\n",
    "pos -= mean\n",
    "\n",
    "surf = ax.pcolormesh(y1, y2, y2)\n",
    "lin, = ay.plot(y, y)\n",
    "fig.colorbar(surf, label='PDF')\n",
    "\n",
    "@ipywidgets.interact\n",
    "def plot_gauss(correlation=(-0.99, 0.99, 0.01)):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    covi = np.linalg.inv(cov)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    \n",
    "    pdf = np.exp(-1/2 * np.sum(pos @ covi * pos, axis=-1))\n",
    "    pdf *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov))\n",
    "    \n",
    "    pdfx = np.trapz(pdf, axis=0)\n",
    "    \n",
    "    lin.set_data(y, pdfx)\n",
    "    ay.relim()\n",
    "    ay.autoscale_view()\n",
    "\n",
    "    surf.set_array(pdf)\n",
    "    surf.set_clim([pdf.min(), pdf.max()])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477cb71",
   "metadata": {},
   "source": [
    "### Conditioning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d58e7",
   "metadata": {},
   "source": [
    "Conditioning is the process of fixing a subset of the random variables and looking at the distribution of the other variables.\n",
    "\n",
    "Consider the same setup defined above.\n",
    "\n",
    "$$\\begin{pmatrix}\\vect{x} \\\\ \\vect{y}\\end{pmatrix} \\sim\n",
    "\\mathcal{N}\\left(\n",
    "    \\begin{pmatrix}\\vect{\\mu_x} \\\\ \\vect{\\mu_y}\\end{pmatrix},\n",
    "    \\begin{pmatrix}\\vect{\\Sigma_{xx}} & \\vect{\\Sigma_{xy}}\\\\ \\vect{\\Sigma_{xy}}^T & \\vect{\\Sigma_{yy}}\\end{pmatrix}\\right) $$\n",
    "\n",
    "The conditional probability of $\\vect{x}$ when $\\vect{y}$ is fixed is given by:\n",
    "\n",
    "$$P(\\vect{x}|\\vect{y}=\\vect{a}) = \\mathcal{N}\\left(\\vect{\\mu_x} + \\vect{\\Sigma_{xy}}\\vect{\\Sigma_{yy}}^{-1}(\\vect{a}-\\vect{\\mu_y}),\\,\\, \\vect{\\Sigma_{xx}} - \\vect{\\Sigma_{xy}}\\vect{\\Sigma_{yy}}^{-1}\\vect{\\Sigma_{xy}}^T \\right)$$\n",
    "\n",
    "Note how in this case both, the mean and covariance, of the posterior on $\\vect{x}$ are changed.\n",
    "\n",
    "The example below show this fact for a 2D case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889438c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:37:15.962589Z",
     "start_time": "2023-02-03T18:37:15.780438Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('x')\n",
    "ax.set_xlabel('y')\n",
    "ax.grid(False)\n",
    "ay.set_xlabel('x')\n",
    "ay.set_ylabel('P(x|y)')\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y = np.linspace(-4, 4, 200)\n",
    "y1, y2 = np.meshgrid(y, y)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "mean = np.array([0.0, 0.0])\n",
    "pos -= mean\n",
    "\n",
    "surf = ax.pcolormesh(y1, y2, y2)\n",
    "hlin = ax.axline((0, 0), (1, 0), lw=1, ls='--', color='k')\n",
    "lin, = ay.plot(y, y)\n",
    "fig.colorbar(surf, label='PDF')\n",
    "\n",
    "@ipywidgets.interact\n",
    "def plot_gauss(correlation=(-0.99, 0.99, 0.01), y0=(-4, 4, 0.01)):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    covi = np.linalg.inv(cov)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    \n",
    "    pdf = np.exp(-1/2 * np.sum(pos @ covi * pos, axis=-1))\n",
    "    pdf *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov))\n",
    "    \n",
    "    sigx = cov[0, 0] - correlation**2/cov[1, 1]\n",
    "    mux = 0 + correlation/cov[1, 1]*(y0-0)\n",
    "    pdfx = np.exp(-1/2 * (y-mux)**2/sigx**2)\n",
    "    pdfx /= np.sqrt(2 * np.pi * sigx**2)\n",
    "    lin.set_data(y, pdfx)\n",
    "    ay.relim()\n",
    "    ay.autoscale_view()\n",
    "    \n",
    "    global hlin\n",
    "    hlin.remove()\n",
    "    hlin = ax.axline((0, y0), (1, y0), lw=1, ls='--', color='k')\n",
    "    surf.set_array(pdf)\n",
    "    surf.set_clim([pdf.min(), pdf.max()])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddd5de",
   "metadata": {},
   "source": [
    "### Affine Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183bcc1",
   "metadata": {},
   "source": [
    "An affine transformation of a random variable $\\vect{x}\\in\\mathbb{R}^N$ into another random variable $\\vect{y}\\in\\mathbb{R}^M$ is defined by:\n",
    "\n",
    "$$ \\vect{y} = \\vect{c} + \\vect{B}\\vect{x}$$\n",
    "\n",
    "where $\\vect{c} \\in \\mathbb{R}^M$ and $\\vect{B}\\in\\mathbb{R}^M\\times\\mathbb{R}^N$ are constants.\n",
    "\n",
    "Given that $\\vect{x}$ is distributed normally, $\\vect{x}\\sim\\mathcal{N}(\\vect{\\mu_x},\\, \\vect{\\Sigma})$, then $\\vect{y}$ will also be distributed normally:\n",
    "\n",
    "$\\vect{y}\\sim\\mathcal{N}(\\vect{c} + \\vect{B}\\vect{\\mu_x},\\, \\vect{B}\\vect{\\Sigma}\\vect{B}^T)$\n",
    "\n",
    "\n",
    "As an example, let's look to the 2D case below, where $\\vect{B}=(a, b)$, with $a,b\\in\\mathbb{R}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d095c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:59:10.311971Z",
     "start_time": "2023-02-03T18:59:10.183674Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.set_xlabel('$x_2$')\n",
    "ax.grid(False)\n",
    "ay.set_xlabel('$y = c + ax_1 + bx_2$')\n",
    "ay.set_ylabel('P(y)')\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y = np.linspace(-4, 4, 200)\n",
    "y1, y2 = np.meshgrid(y, y)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "mean = np.array([0.0, 0.0])\n",
    "pos -= mean\n",
    "\n",
    "surf = ax.pcolormesh(y1, y2, y2)\n",
    "hlins = [\n",
    "    ax.axline((0, 0), slope=0, lw=1, ls='--', color='k')\n",
    "    for yi in np.linspace(-8, 8, 17)]\n",
    "lin, = ay.plot(y, y)\n",
    "fig.colorbar(surf, label='PDF')\n",
    "\n",
    "@ipywidgets.interact\n",
    "def plot_gauss(\n",
    "        correlation=(-0.99, 0.99, 0.01), a=(-10, 10, 0.01), b=(0.01, 1, 0.01),\n",
    "        c=(-4, 4, 0.01)):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    covi = np.linalg.inv(cov)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    \n",
    "    pdf = np.exp(-1/2 * np.sum(pos @ covi * pos, axis=-1))\n",
    "    pdf *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov))\n",
    "    \n",
    "    B = np.array([a, b])\n",
    "    sigy = B @ cov @ B.T\n",
    "    muy = c + B @ np.array([0, 0])\n",
    "    pdfy = np.exp(-1/2 * (y-muy)**2/sigy**2)\n",
    "    pdfy /= np.sqrt(2 * np.pi * sigy**2)\n",
    "    lin.set_data(y, pdfy)\n",
    "    ay.relim()\n",
    "    ay.autoscale_view()\n",
    "    \n",
    "    global hlins\n",
    "    _ = [h.remove() for h in hlins]\n",
    "    hlins = [\n",
    "        ax.axline((0, (yi-c)/b), slope=-a/b, lw=1, ls='--', color='w')\n",
    "        for yi in np.linspace(-8, 8, 17)]\n",
    "    surf.set_array(pdf)\n",
    "    surf.set_clim([pdf.min(), pdf.max()])\n",
    "    ax.set_xlim([-4, 4])\n",
    "    ax.set_ylim([-4, 4])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e92edb",
   "metadata": {},
   "source": [
    "### Product of two distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182ed2f",
   "metadata": {},
   "source": [
    "The multivariate gaussian is also closed under multiplication. \n",
    "\n",
    "Given two normal distributions\n",
    "\n",
    "$$\\mathcal{N}(\\vect{\\mu_1},\\, \\vect{\\Sigma_1})\\quad\\mathcal{N}(\\vect{\\mu_2},\\, \\vect{\\Sigma_2}),$$\n",
    "\n",
    "the product between them is also a gaussian:\n",
    "\n",
    "$$\\mathcal{N}(\\vect{\\mu_3},\\, \\vect{\\Sigma_3})=\\mathcal{N}(\\vect{\\mu_1},\\, \\vect{\\Sigma_1})\\mathcal{N}(\\vect{\\mu_2},\\, \\vect{\\Sigma_2}),$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\vect{\\Sigma_3} = \\vect{\\Sigma_1}\\left(\\vect{\\Sigma_1} + \\vect{\\Sigma_2}\\right)^{-1}\\vect{\\Sigma_2}$$\n",
    "$$\\vect{\\mu_3} = \n",
    "\\vect{\\Sigma_2}\\left(\\vect{\\Sigma_1} + \\vect{\\Sigma_2}\\right)^{-1}\\vect{\\mu_1} + \n",
    "\\vect{\\Sigma_1}\\left(\\vect{\\Sigma_1} + \\vect{\\Sigma_2}\\right)^{-1}\\vect{\\mu_2}.$$\n",
    "\n",
    "Let's again look at a 2D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e217c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:37:18.169824Z",
     "start_time": "2023-02-03T18:37:18.066622Z"
    },
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.set_xlabel('$x_2$')\n",
    "ax.grid(False)\n",
    "ay.set_xlabel('$x_1$')\n",
    "ay.set_ylabel('$x_2$')\n",
    "ay.grid(False)\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y = np.linspace(-4, 4, 200)\n",
    "y1, y2 = np.meshgrid(y, y)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "surf1 = ax.pcolormesh(y1, y2, y2, alpha=0.3, cmap='jet')\n",
    "surf2 = ax.pcolormesh(y1, y2, y2, alpha=0.5, cmap='copper')\n",
    "surf3 = ay.pcolormesh(y1, y2, y2, alpha=1, cmap='jet')\n",
    "\n",
    "def plot_gauss(\n",
    "        corr1=0, mu1_1=0, mu1_2=0,\n",
    "        corr2=0, mu2_1=0, mu2_2=0,\n",
    "        calculated=False):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    mu1 = np.array([mu1_1, mu1_2])\n",
    "    cov1 = np.eye(2)\n",
    "    cov1[0, 1] = corr1\n",
    "    cov1[1, 0] = corr1\n",
    "\n",
    "    mu2 = np.array([mu2_1, mu2_2])\n",
    "    cov2 = np.eye(2)\n",
    "    cov2[0, 1] = corr2\n",
    "    cov2[1, 0] = corr2\n",
    "\n",
    "    inv = np.linalg.inv(cov1 + cov2)\n",
    "    cov3 = cov1 @ inv @ cov2\n",
    "    mu3 = cov2 @ inv @ mu1 + cov1 @ inv @ mu2\n",
    "    \n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    cov1i = np.linalg.inv(cov1)\n",
    "    cov2i = np.linalg.inv(cov2)\n",
    "    cov3i = np.linalg.inv(cov3)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    pdf1 = np.exp(-1/2 * np.sum((pos-mu1) @ cov1i * (pos-mu1), axis=-1))\n",
    "    pdf1 *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov1))\n",
    "    \n",
    "    pdf2 = np.exp(-1/2 * np.sum((pos-mu2) @ cov2i * (pos-mu2), axis=-1))\n",
    "    pdf2 *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov2))\n",
    "    \n",
    "    if calculated:\n",
    "        pdf3 = np.exp(-1/2 * np.sum((pos-mu3) @ cov3i * (pos-mu3), axis=-1))\n",
    "        pdf3 *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov3))\n",
    "        surf3.set_cmap('copper')\n",
    "    else:\n",
    "        pdf3 = pdf1 * pdf2\n",
    "        surf3.set_cmap('jet')\n",
    "    \n",
    "    surf1.set_array(pdf1)\n",
    "    surf1.set_clim([pdf1.min(), pdf1.max()])\n",
    "\n",
    "    surf2.set_array(pdf2)\n",
    "    surf2.set_clim([pdf2.min(), pdf2.max()])\n",
    "\n",
    "    surf3.set_array(pdf3)\n",
    "    surf3.set_clim([pdf3.min(), pdf3.max()])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "wid = ipywidgets.interactive(\n",
    "    plot_gauss,\n",
    "    corr1=(-0.99, 0.99, 0.01), mu1_1=(-4, 4, 0.01), mu1_2=(-4, 4, 0.01),\n",
    "    corr2=(-0.99, 0.99, 0.01), mu2_1=(-4, 4, 0.01), mu2_2=(-4, 4, 0.01),\n",
    "    calculated=False)\n",
    "\n",
    "controls = ipywidgets.HBox(\n",
    "    wid.children[:-1],\n",
    "    layout=ipywidgets.Layout(flex_flow='row wrap'))\n",
    "output = wid.children[-1]\n",
    "display(ipywidgets.VBox([controls, output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c0faf",
   "metadata": {},
   "source": [
    "Note that all the properties described above can be calculated using very basic linear algebra, which implies that almost everything involving gaussians is very easy and fast to evaluate.\n",
    "\n",
    "These properties are the main reason gaussians are used everywhere in statistics, mainly in Bayesian Inference.\n",
    "\n",
    "What generally requires integration in a high dimensional space with other distributions can be accomplished with a few matrix multiplications when gaussians are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f572c3f",
   "metadata": {},
   "source": [
    "## A Different Way to Visualize Samples from Multivariate Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542733e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T20:27:26.471500Z",
     "start_time": "2023-02-03T20:27:26.466777Z"
    },
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def kernel_squared_exponential(x, leng=1, sigma=1):\n",
    "    x = np.array(x, ndmin=2)\n",
    "    leng = np.array(leng, ndmin=1)\n",
    "    leng *= np.sqrt(2)\n",
    "    x /= leng[:, None]\n",
    "    dx = x[..., None] - x[..., None, :]\n",
    "    dx2 = np.einsum('i...,i...->...', dx, dx)\n",
    "    return sigma*np.exp(-dx2)\n",
    "\n",
    "\n",
    "def kernel_exponential(x, leng=1, sigma=1):\n",
    "    x = np.array(x, ndmin=2)\n",
    "    leng = np.array(leng, ndmin=1)\n",
    "    x /= leng[:, None]\n",
    "    dx = x[..., None] - x[..., None, :]\n",
    "    dxa = np.sqrt(np.einsum('i...,i...->...', dx, dx))\n",
    "    return sigma*np.exp(-dxa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c96cdc",
   "metadata": {},
   "source": [
    "In order to build a more intuitive view of a gaussian process, let's first remember the multivariate gaussian distribution:\n",
    "\n",
    "$$\\vect{y} \\sim \\mathcal{N}(\\vect{\\mu}, \\vect{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^N|\\vect{\\Sigma}|}}\\exp\\left(-\\frac12(\\vect{y}-\\vect{\\mu})^T\\vect{\\Sigma}^{-1}(\\vect{y}-\\vect{\\mu})\\right)$$\n",
    "\n",
    "whose symbols meaning are the same as discussed previously.\n",
    "\n",
    "The usual way of visualizing this distribution is by plotting its PDF or drawing samples from the distribution and plotting one versus another.\n",
    "\n",
    "In the example below we will visualize a 2D gaussian differently. This approach will help us see the influence of the covariance between the random variables and extend the visualization into higher dimensions.\n",
    "\n",
    "Notice that when we increase the correlation between both variables the slope of lines on the right become smaller and the line approaches the horizontal line. On the other hand, when we make a high negative correlation the slopes become closer to $\\pm1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6952f23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:37:21.349377Z",
     "start_time": "2023-02-03T18:37:21.272433Z"
    },
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ay, ax) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('index')\n",
    "\n",
    "line, = ay.plot([0], [0], 'o')\n",
    "ay.set_ylabel(r'$y_1$')\n",
    "ay.set_xlabel(r'$y_0$')\n",
    "\n",
    "@ipywidgets.interact(correlation=(-0.999, 0.999, 0.001), n_samples=(1, 100, 1))\n",
    "def plot_gauss(correlation=0, n_samples=1):\n",
    "    [l.remove() for l in ax.lines]\n",
    "    x = np.linspace(0, 1, 2)\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "    mean = np.zeros(x.size)\n",
    "    data = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    ax.plot(x, data.T, lw=1)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    line.set_data(data[:, 0], data[:, 1])\n",
    "    ay.relim()\n",
    "    ay.autoscale_view()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58855364",
   "metadata": {},
   "source": [
    "When we go to higher dimensions it is difficult to visualize the variables the same way we did on the left, but the graph on the right can be readly applied for an arbitrary number of dimensions.\n",
    "\n",
    "In the figure below we plot five samples of the $N$-dimensional vector of random variables as function of the coordinate index $i$. Note how the curves change as we increase the correlation between the several coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5b995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:56:14.908460Z",
     "start_time": "2023-02-03T18:56:14.855253Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = mplt.subplots(1, 1, figsize=(7, 3))\n",
    "ax.set_ylabel(r'$y$')\n",
    "ax.set_xlabel('index')\n",
    "\n",
    "@ipywidgets.interact(correlation=(1e-3, 0.999, 0.001), ndim=(2, 500, 1))\n",
    "def plot_gauss(correlation=1e-3, ndim=2):\n",
    "    ncurves=5\n",
    "    [l.remove() for l in ax.lines]\n",
    "    x = np.arange(ndim, dtype=float)/ndim\n",
    "    cov = kernel_squared_exponential(x, leng=correlation)\n",
    "    mean = np.zeros(x.size)\n",
    "    data = np.random.multivariate_normal(mean, cov, size=ncurves)\n",
    "    ax.plot(data.T, lw=1)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ba508",
   "metadata": {},
   "source": [
    "I was not completely honest in the graph above, because when we are in 2 dimensions there is only one way we can increase the correlation between the two variables. However when we go to higher dimensions, $N$, there are $N(N-1)/2$ free coefficients in the covariance matrix to define the covariance between each pair of coordinates. In the graph above I changed the covariance matrix in a very specific way:\n",
    "\n",
    "$$\\Sigma_{ij} = K(i, j) := \\exp\\left(-\\frac12\\left(\\frac{i-j}{Nl}\\right)^2\\right)$$\n",
    "\n",
    "where $K(i,j)$ is called kernel function and $l$ is the variable we change in the knob of the graphic.\n",
    "\n",
    "Note that the self-correlation, or the variance of every varible, is $K(i, i)=\\exp(0)=1$ and the correlation between variables decays as they are more apart in the index set.\n",
    "\n",
    "If I used another kernel we would have a very different figure. For example, consider the following covariance matrix:\n",
    "\n",
    "$$\\Sigma_{ij} = K(i, j) := \\exp\\left(-\\frac{|i-j|}{Nl}\\right)$$\n",
    "\n",
    "we get the following behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66d8ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:55:26.288769Z",
     "start_time": "2023-02-03T18:55:26.238011Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = mplt.subplots(1, 1, figsize=(7, 3))\n",
    "ax.set_ylabel(r'$y$')\n",
    "ax.set_xlabel('index')\n",
    "\n",
    "@ipywidgets.interact(correlation=(1e-3, 0.999, 0.001), ndim=(2, 500, 1))\n",
    "def plot_gauss(correlation=1e-3, ndim=2):\n",
    "    ncurves=5\n",
    "    [l.remove() for l in ax.lines]\n",
    "    x = np.arange(ndim, dtype=float)/ndim\n",
    "    cov = kernel_exponential(x, leng=correlation)\n",
    "    mean = np.zeros(x.size)\n",
    "    data = np.random.multivariate_normal(mean, cov, size=ncurves)\n",
    "    ax.plot(data.T, lw=1)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c269d2",
   "metadata": {},
   "source": [
    "which is continuous but not differentiable and is compatible with the brownian motion of a particle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdf6f3",
   "metadata": {},
   "source": [
    "# Bayes Theorem and Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182c99b",
   "metadata": {},
   "source": [
    "Suppose we have a physical quantity $y$ whose value we don't know but we think it is distributed normally with $\\sigma=3$. Since we don't know the value, we assume $\\mu=0$. The PDF of this variable is given by:\n",
    "\n",
    "$$P(y|\\mu=0,\\sigma=3) = \\mathcal{N}(0,3) = \\frac{1}{3\\sqrt{2\\pi}}\\exp\\left(-\\frac12\\frac{y^2}{3^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f9289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:37:31.588520Z",
     "start_time": "2023-02-03T18:37:31.532098Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code made by chatgpt with a few modifications\n",
    "\n",
    "# Define the prior mean and standard deviation\n",
    "mu_0 = 0\n",
    "sigma_0 = 3\n",
    "\n",
    "# Define a range of X values\n",
    "X = np.linspace(-20, 20, 10000)\n",
    "\n",
    "# Calculate the prior PDF\n",
    "prior_pdf = scystat.norm.pdf(X, mu_0, sigma_0)\n",
    "\n",
    "# Plot the prior PDF\n",
    "fig, ax = mplt.subplots(1, 1, figsize=(5, 3))\n",
    "ax.plot(X, prior_pdf, label='Prior')\n",
    "ax.set_xlabel('y')\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.legend(loc='best')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ccac3",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Now suppose we make a measurement $y_0$ of this quantity and the value is $5$. The measurement procedure has an uncertainty whose standard deviation is $1$. We could say that the result of the measurement also is a gaussian:\n",
    "\n",
    "$$P(y_0|y, \\mu_0=5, \\sigma_0=1) = \\mathcal{N}(5, 1) = \\frac{1}{1\\sqrt{2\\pi}}\\exp\\left(-\\frac12\\frac{(y_0-5)^2}{1^2}\\right)$$\n",
    "\n",
    "we can use Bayes Theorem to update our beliefs given this new information:\n",
    "\n",
    "$$P(y|y_0) = \\frac{P(y_0|y)P(y)}{P(y_0)}$$\n",
    "\n",
    "where :\n",
    " - $P(y|y_0)$ is called the posterior, because it represents our updated beliefs on $y$ after the measurement;\n",
    " - $P(y)=P(y|\\mu,\\sigma)$ is our prior assumption;\n",
    " - $P(y_0|y)$ called likelihood, which in our case is the measurement information;\n",
    " - $P(y_0)$ is the probability of having measured $y_0$ (marginal likelihood) which here we can consider just as a normalization constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7522dc93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T20:27:26.561515Z",
     "start_time": "2023-02-03T20:27:26.472667Z"
    },
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def animate_bayes(frames, mu_x, sigma_x, x, prior):\n",
    "    post_pdf = prior.copy()\n",
    "\n",
    "    # Plot the prior PDF\n",
    "    fig, ax = mplt.subplots(1, 1, figsize=(5, 3))\n",
    "    ax.plot(X, prior, label='Prior')\n",
    "    ax.set_xlabel('y')\n",
    "    ax.set_ylabel('Probability density')\n",
    "    ax.legend(loc='best')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    line, = ax.plot([0], [0])\n",
    "    lines = ax.plot([0], np.array([[0]]*len(frames)).T)\n",
    "    for l in lines:\n",
    "        l.set_visible(False)\n",
    "    def update(i, post):\n",
    "        mux = mu_x[i]\n",
    "        sigx = sigma_x[i]\n",
    "        # Calculate the likelihood PDF\n",
    "        likelihood_pdf = scystat.norm.pdf(x, mux, sigx)\n",
    "\n",
    "        # Plot the likelihood PDF\n",
    "        lines[i].set_data(x, likelihood_pdf)\n",
    "        lines[i].set_label(f'y{i:d}~N({mux:.1f},{sigx:.1f})')\n",
    "        lines[i].set_visible(True)\n",
    "\n",
    "        # Multiply the prior PDF and the likelihood PDF to \n",
    "        # get the unnormalized posterior PDF\n",
    "        post *= likelihood_pdf\n",
    "\n",
    "        # Normalize the unnormalized posterior PDF\n",
    "        norm_constant = np.trapz(post, x)\n",
    "        post /= norm_constant\n",
    "\n",
    "        # Plot the posterior PDF\n",
    "        line.set_data(x, post)\n",
    "        line.set_label(f'Posterior {i:d}')\n",
    "        ax.legend(loc='best', fontsize='x-small')\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        fig.tight_layout()\n",
    "        return []\n",
    "\n",
    "    return FuncAnimation(\n",
    "        fig, update, fargs=(post_pdf, ), frames=frames,\n",
    "        repeat=False, repeat_delay=2, interval=1000, init_func=lambda: [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fed84f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T20:27:26.729154Z",
     "start_time": "2023-02-03T20:27:26.566688Z"
    },
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Define the measurement mean and standard deviation\n",
    "mu_x = [5, 1, 3, 2, 3.5]\n",
    "sigma_x = [1, 4, 1.5, 2, 0.5]\n",
    "frames = list(range(len(mu_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4914c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:37:43.808694Z",
     "start_time": "2023-02-03T18:37:43.747087Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = animate_bayes(frames[:1], mu_x, sigma_x, X, prior_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06a0d8",
   "metadata": {},
   "source": [
    "If more data is measured our beliefs can be updated considering the previous posterior distribution as prior:\n",
    "\n",
    "$$P(y|\\{y_0,y_1\\}) = \\frac{P(y_1|y)P(y|y_0)}{P(y_1)} = \\frac{P(y_1|y)P(y_0|y)P(y)}{P(y_1)P(y_0)}$$\n",
    "\n",
    "This process can be repeated indefinetly considering new measurements are made:\n",
    "\n",
    "$$P(y|\\mathcal{D}) = \\frac{\\prod_\\mathcal{D}P(y_i|y)}{\\prod_\\mathcal{D}P(y_i)}P(y), \\quad \\mathcal{D}=\\{y_i| i=0..N\\} \\quad\\mathrm{and}\\quad y_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22d690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:38:16.226721Z",
     "start_time": "2023-02-03T18:38:16.183118Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim = animate_bayes(frames, mu_x, sigma_x, X, prior_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6906251",
   "metadata": {},
   "source": [
    "It is worth noticing that the final posterior does not depend on the order of the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc5c9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:38:29.786234Z",
     "start_time": "2023-02-03T18:38:29.667297Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim2 = animate_bayes(\n",
    "    np.random.permutation(frames), mu_x, sigma_x, X, prior_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76191bc1",
   "metadata": {},
   "source": [
    "# Parametric Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76706730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T20:27:26.831505Z",
     "start_time": "2023-02-03T20:27:26.736633Z"
    },
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def feature_func_polynomial(x, num_features=2):\n",
    "    \"\"\".\"\"\"\n",
    "    return x.ravel()[:, None]**(np.arange(num_features)[None, :])\n",
    "\n",
    "\n",
    "feature_func_linear = _partial(feature_func_polynomial, num_features=2)\n",
    "\n",
    "\n",
    "def feature_func_sines(x, num_freqs=6, L=1):\n",
    "    mat = np.empty((x.size, num_freqs*2-1), dtype=float)\n",
    "    x_mat = x.ravel()[:, None]/L\n",
    "    x_mat = x_mat * np.arange(num_freqs)[None, :]\n",
    "    mat[:, :num_freqs] = np.sin(x_mat)\n",
    "    mat[:, num_freqs:] = np.cos(x_mat[:, 1:])\n",
    "    return mat\n",
    "\n",
    "\n",
    "def feature_func_gauss(x, mean_range=6, sigma=1):\n",
    "    means = np.arange(-mean_range, mean_range+1)\n",
    "    mat = np.exp(-(x.ravel()[:, None]-means[None, :])**2/2/sigma**2)\n",
    "    return mat\n",
    "\n",
    "\n",
    "def feature_func_steps(x, step_range=6):\n",
    "    steps = np.arange(-step_range, step_range+1)\n",
    "    mat = (x.ravel()[:, None] > steps[None, :])*2 - 1\n",
    "    return mat\n",
    "\n",
    "\n",
    "def parametric_bayesian_regression(\n",
    "        x, x_data, y_data, prior, feature_func, sigma_err=1):\n",
    "    phi_x = feature_func(x)\n",
    "    phi_data = feature_func(x_data)\n",
    "    num_features = phi_data.shape[1]\n",
    "    mu_prior = prior.mean[:, None]\n",
    "    sigma_prior = prior.cov_object.covariance\n",
    "\n",
    "    muf_data = phi_data @ mu_prior\n",
    "    kXX = phi_data @ sigma_prior @ phi_data.T  # kXX\n",
    "    err_data = y_data[:, None]-muf_data\n",
    "\n",
    "    if x_data.size < num_features:\n",
    "        # this method has the advantage of easy interpretation of a \n",
    "        # joint distribution between measured data and infered data and also\n",
    "        # shows more clearly the importance of the kernel function\n",
    "        kXXi = np.linalg.inv(kXX + np.eye(x_data.size)*sigma_err**2)\n",
    "        B = sigma_prior @ phi_data.T\n",
    "        mu_post = mu_prior + B @ kXXi @ err_data\n",
    "        sigma_post = sigma_prior - B @ kXXi @ B.T\n",
    "    else:\n",
    "        # this method is optimized for larger sets of data, due to the fact \n",
    "        # that the matrix inversion is done in the parameters covariance \n",
    "        # matrix, which is generally smaller than the data matrix.\n",
    "        A = phi_data.T @ phi_data/sigma_err**2 + np.linalg.inv(sigma_prior)\n",
    "        sigma_post = np.linalg.inv(A)\n",
    "        mu_post = mu_prior + 1/sigma_err**2*sigma_post @ phi_data.T @ err_data\n",
    "    return scystat.multivariate_normal(mu_post.ravel(), sigma_post)\n",
    "\n",
    "\n",
    "def animate_regression(\n",
    "        x, x_data, y_data, prior, feature_func, sigma_err=1, truth=None):\n",
    "\n",
    "    post = parametric_bayesian_regression(\n",
    "        x, x_data, y_data, prior, feature_func, sigma_err)\n",
    "    dist = prior\n",
    "    phix = feature_func(x)\n",
    "    muf = phix @ dist.mean\n",
    "    stdf = phix @ dist.cov_object.covariance @ phix.T\n",
    "    stdf = np.sqrt(np.diag(stdf))\n",
    "    \n",
    "    is2d = len(dist.mean) == 2\n",
    "    if is2d:\n",
    "        fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "        wgrid = np.linspace(-6, 6, 100)\n",
    "        w1grid, w2grid = np.meshgrid(wgrid, wgrid)\n",
    "        pos = np.empty(w1grid.shape + (2, ))\n",
    "        pos[:, :, 0] = w1grid\n",
    "        pos[:, :, 1] = w2grid\n",
    "    else:\n",
    "        fig, ay = mplt.subplots(1, 1, figsize=(5, 3))\n",
    "   \n",
    "    def animate(frm):\n",
    "        dist = parametric_bayesian_regression(\n",
    "            x, x_data[:frm], y_data[:frm], prior, feature_func, sigma_err)\n",
    "        if is2d:\n",
    "            pdf = dist.pdf(pos)\n",
    "            ax.clear()\n",
    "            ax.grid(False)\n",
    "            levels = pdf.max()*np.array([0.1, 0.4, 0.7, 0.95])\n",
    "            ax.contour(\n",
    "                w1grid, w2grid, pdf, cmap='copper', levels=levels)\n",
    "            ax.pcolormesh(w1grid, w2grid, pdf, cmap='copper', alpha=0.5)\n",
    "            if truth is not None and not isinstance(truth[0], np.ndarray):\n",
    "                ax.plot(*truth, 'ko', label='Truth')\n",
    "\n",
    "            ax.legend(loc='lower right')\n",
    "            ax.set_xlabel('w1 (intercept)')\n",
    "            ax.set_ylabel('w2 (angular coeff)')\n",
    "            ax.set_xlim([-3, 3])\n",
    "            ax.set_ylim([-3, 3])\n",
    "            \n",
    "        muf = phix @ dist.mean\n",
    "        stdf = phix @ dist.cov_object.covariance @ phix.T\n",
    "        stdf = np.sqrt(np.diag(stdf))\n",
    "        \n",
    "        ay.clear()\n",
    "        ay.set_xlabel('x')\n",
    "        ay.set_ylabel('y = f(x) + epsilon')\n",
    "        if truth is not None:\n",
    "            if not isinstance(truth[0], np.ndarray):\n",
    "                ay.plot(x, phix@truth, 'k--', label='Truth')\n",
    "            else:\n",
    "                ay.plot(truth[0], truth[1], 'k--', label='Truth')\n",
    "        \n",
    "        ay.plot(x, muf, label='expected f(x)')\n",
    "        ay.fill_between(\n",
    "            x, muf+1.96*stdf, muf-1.96*stdf, color='C0', alpha=0.2,\n",
    "            label='95% confidence')\n",
    "        ay.errorbar(\n",
    "            x_data[:frm], y_data[:frm], yerr=sigma_err, linestyle='',\n",
    "            marker='o', color='k', barsabove=True, label='Data')\n",
    "        ay.legend(loc='best', fontsize='x-small')\n",
    "        fig.tight_layout()\n",
    "        return []\n",
    "    \n",
    "#     return animate(0)\n",
    "    return FuncAnimation(\n",
    "        fig, animate, frames=np.arange(x_data.size+1),\n",
    "        repeat=True, repeat_delay=1000, interval=1000)\n",
    "\n",
    "\n",
    "def animate_distribution(x, dist, feature_func):\n",
    "    \"\"\".\"\"\"\n",
    "    frames = np.arange(40)\n",
    "    phix = feature_func(x)\n",
    "    muf = phix @ dist.mean\n",
    "    stdf = np.sqrt(np.diag(phix @ dist.cov_object.covariance @ phix.T))\n",
    "\n",
    "    # Create rotating samples from the distribution:\n",
    "    # First define the levels we want:\n",
    "    lev_smpl = -2*np.log([0.9, 0.7, 0.5, 0.36])\n",
    "    # Create normalized random vectors for rotation:\n",
    "    N = phix.shape[1]\n",
    "    nvec = lev_smpl.size\n",
    "    v = np.random.randn(N, nvec, 2)  # create 2 lists of nvec random vectors\n",
    "    v[..., 0] /= np.linalg.norm(v[..., 0], axis=0)  # normalize first list\n",
    "    # make ith vector of second list orthogonal to ith vector of first list:\n",
    "    v[..., 1] -= v[..., 0]*np.sum(v[..., 0]*v[..., 1], axis=0)\n",
    "    v[..., 1] /= np.linalg.norm(v[..., 1], axis=0)  # normalize second list\n",
    "\n",
    "    # Create transformation matrix from v to w\n",
    "    L = np.linalg.cholesky(dist.cov_object.covariance)\n",
    "    # Create the parametrized rotation matrix:\n",
    "    R = lambda x: np.array([[np.cos(x)], [np.sin(x)]])\n",
    "    \n",
    "    is2d = len(dist.mean) == 2\n",
    "    if is2d:\n",
    "        fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "        sigm = max(*np.diag(dist.cov_object.covariance), 0.5)\n",
    "        lims1 = dist.mean[0] - 2.1*sigm, dist.mean[0] + 2.1*sigm\n",
    "        lims2 = dist.mean[1] - 2.1*sigm, dist.mean[1] + 2.1*sigm\n",
    "        \n",
    "        w1grid, w2grid = np.meshgrid(\n",
    "            np.linspace(*lims1, 100), np.linspace(*lims2, 100))\n",
    "        pos = np.empty(w1grid.shape + (2, ))\n",
    "        pos[:, :, 0] = w1grid\n",
    "        pos[:, :, 1] = w2grid\n",
    "        pdf = dist.pdf(pos)\n",
    "        lev_cont = pdf.max()*np.array([0.1, 0.4, 0.7, 0.95])\n",
    "    else:\n",
    "        fig, (ax, ay) = mplt.subplots(\n",
    "            2, 1, figsize=(5, 3), sharex=True, height_ratios=[1, 3])\n",
    "        ax.plot(x, phix, 'k', lw=1)\n",
    "        ax.set_ylabel('Basis')\n",
    "   \n",
    "    def animate(frame):\n",
    "        # Rotate and transform the vector\n",
    "        theta = 2*np.pi * frame/frames.size\n",
    "        vr = (v @ R(theta)).squeeze()\n",
    "        vr *= lev_smpl[None, :]\n",
    "        wr = dist.mean[:, None] + L @ vr\n",
    "\n",
    "        if is2d:\n",
    "            ax.clear()\n",
    "            ax.grid(False)\n",
    "            ax.contour(\n",
    "                w1grid, w2grid, pdf, cmap='copper', levels=lev_cont)\n",
    "            ax.pcolormesh(\n",
    "                w1grid, w2grid, pdf, cmap='copper', alpha=0.5)\n",
    "            for i, dum in enumerate(wr.T, 1):\n",
    "                ax.plot(*dum, 'o', color=f'C{i:d}')\n",
    "            ax.set_xlabel(r'$\\omega_1$ (intercept)')\n",
    "            ax.set_ylabel(r'$\\omega_2$ (angular coeff)')\n",
    "            sigm = max(*np.diag(dist.cov_object.covariance), 0.5)\n",
    "            ax.set_xlim(lims1)\n",
    "            ax.set_ylim(lims2)\n",
    "            \n",
    "        muf = phix @ dist.mean\n",
    "        stdf = phix @ dist.cov_object.covariance @ phix.T\n",
    "        stdf = np.sqrt(np.diag(stdf))\n",
    "        \n",
    "        ay.clear()\n",
    "        ay.set_xlabel('x')\n",
    "        ay.set_ylabel(r'$y = f(x) + \\varepsilon$')\n",
    "        \n",
    "        ay.plot(x, muf, label=r'$\\mathbb{E}(f(x))$')\n",
    "        ay.fill_between(\n",
    "            x, muf+1.96*stdf, muf-1.96*stdf, color='C0', alpha=0.2,\n",
    "            label='95% confidence')\n",
    "        lines = [\n",
    "            ay.plot(\n",
    "                x, xd, '--', color=f'C{i:d}',lw=1)[0]\n",
    "            for i, xd in enumerate((phix @ wr).T, 1)]\n",
    "        lines[0].set_label('samples')\n",
    "\n",
    "        ay.legend(\n",
    "            loc='lower center', bbox_to_anchor=(0.5, 0), fontsize='small')\n",
    "        fig.tight_layout()\n",
    "        return []\n",
    "    \n",
    "#     return animate(0)\n",
    "    return FuncAnimation(\n",
    "        fig, animate, frames=frames,\n",
    "        repeat=True, repeat_delay=20, interval=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b4678",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "As an example let's see how we could use the framework defined above to perform parametric regression of some curves.\n",
    "\n",
    "First, let's start with the simple straight line model:\n",
    "\n",
    "$$ f(x) = \\omega_1 + \\omega_2x = \\vect{\\phi}(x)^T\\vect{\\omega}$$\n",
    "\n",
    "where $\\vect{\\omega} \\in \\mathbb{R}^2$ is the vector of random variables we want to know more about and $\\vect{\\phi}(x) = (1, x)^T$. When we look this way we see that the function is a random variable defined by the affine transformation $\\vect{\\phi}(x)$ of $\\vect{\\omega}$.\n",
    "\n",
    "Now lets assume a gaussian prior on $\\vect{\\omega}$:\n",
    "\n",
    "$$ \\vect{\\omega} \\sim \\mathcal{N}\\left(\\vect{\\bar{\\omega}}, \\vect{\\Sigma}\\right)$$\n",
    "\n",
    "Considering that f is just a linear combination of $\\vect{\\omega}$, the prior on $f(x)$ is given by:\n",
    "\n",
    "$$ f(x) \\sim \\mathcal{N}\\left(\\vect{\\phi}^T\\vect{\\bar{\\omega}},\\,\\, \\vect{\\phi}^T\\vect{\\Sigma}\\vect{\\phi}\\right)$$\n",
    "\n",
    "The figure below shows the general behavior of this prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234ac0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:40:17.606194Z",
     "start_time": "2023-02-03T18:40:17.602952Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Made up data\n",
    "w_truth = (1, 2)\n",
    "feat_func_truth = _partial(feature_func_polynomial, num_features=len(w_truth))\n",
    "num_data = 10\n",
    "sigma_err = 1\n",
    "x_data = (np.random.rand(num_data)-0.5)*2 * 6\n",
    "y_data = feat_func_truth(x_data) @ w_truth\n",
    "y_data += np.random.randn(*x_data.shape)*sigma_err\n",
    "\n",
    "# points for inference and truth values:\n",
    "num_pts = 100\n",
    "x = np.linspace(-6, 6, num_pts)\n",
    "truth = (x, feat_func_truth(x)@w_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd7176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:40:18.752727Z",
     "start_time": "2023-02-03T18:40:18.747769Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = feature_func_linear\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = 3*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ccb83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:40:19.466166Z",
     "start_time": "2023-02-03T18:40:19.441957Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = animate_distribution(x, prior, feature_func=feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6454c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:41:09.311613Z",
     "start_time": "2023-02-03T18:41:09.308880Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a177e47",
   "metadata": {},
   "source": [
    "Now, suppose that a total of $N$ measurements are performed, such that:\n",
    "\n",
    "$$\\mathcal{D} = \\{(x_i, y_i)| i\\in[1,N]\\} = (\\vect{x}, \\vect{y})$$\n",
    "\n",
    "where $y_i$ represent noisy observations of the underlying function $f(x_i)$:\n",
    "\n",
    "$$y_i = f(x_i) + \\varepsilon, \\quad\\mathrm{with}\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$$\n",
    "\n",
    "This way we se that the likelyhood of all the measurements is given by:\n",
    "\n",
    "$$P(\\vect{y}|\\vect{\\omega}, \\mathcal{D}) = \\prod_\\mathcal{D}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_i-\\vect{\\phi}(x_i)^T\\vect{\\omega})^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "Since we are considering all measurement error distributions are equally distributed (same $\\sigma^2$) and gaussian. We can simplify the equation above such that:\n",
    "\n",
    "$$P(\\vect{y}|\\vect{\\omega}, \\mathcal{D}) = \\frac{1}{(2\\pi)^{N/2}\\sigma^N}\\exp\\left(-\\frac{|\\vect{y}-\\vect{\\Phi}^T\\vect{\\omega}|^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "where we introduced the matrix $\\vect{\\Phi} := (\\phi(x_1),...\\phi(x_i),...\\phi(x_N))$.\n",
    "\n",
    "The posterior distribution can then be updated using Bayes theorem:\n",
    "\n",
    "$$P(\\vect{\\omega}|\\mathcal{D}) = \\frac{P(\\vect{y}|\\vect{x},\\vect{\\omega})P(\\vect{\\omega})}{P(\\vect{y}|\\vect{x})} \\propto\n",
    "\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(\\vect{y}-\\vect{\\Phi}^T\\vect{\\omega}\\right)^T\\left(\\vect{y}-\\vect{\\Phi}^T\\vect{\\omega}\\right)\\right)\n",
    "\\exp\\left(-\\frac12(\\vect{\\omega}-\\vect{\\bar{\\omega}})^T\\vect{\\Sigma}^{-1}(\\vect{\\omega}-\\vect{\\bar{\\omega}})\\right)\n",
    "$$\n",
    "\n",
    "Since all distributions are gaussian, we can analytically compute the posterior by completing the squares on the equation above. After some math we see the posterior is also gaussian:\n",
    "\n",
    "$$P(\\vect{\\omega}|\\mathcal{D}) = \\mathcal{N}(\\vect{\\bar{\\omega}_p}, \\vect{\\Sigma_p}),\n",
    "\\quad \\mathrm{with} \\quad\n",
    "\\vect{\\bar{\\omega}_p}=\n",
    "\\vect{\\bar{\\omega}} + \\frac{1}{\\sigma^2}\\vect{\\Sigma_p}\\vect{\\Phi}(\\vect{y}-\\vect{\\Phi}^T\\vect{\\bar{\\omega}})\n",
    "\\quad \\mathrm{and} \\quad\n",
    "\\vect{\\Sigma_p}=\\left(\\sigma^{-2}\\vect{\\Phi}\\vect{\\Phi}^T + \\vect{\\Sigma}^{-1}\\right)^{-1}$$\n",
    "\n",
    "and, consequently, the posterior on $f(x)$ is also gaussian:\n",
    "\n",
    "$$ f(x) \\sim \\mathcal{N}\\left(\\vect{\\phi}^T\\vect{\\bar{\\omega}_p},\\,\\, \\vect{\\phi}^T\\vect{\\Sigma_p}\\vect{\\phi}\\right)$$\n",
    "\n",
    "The animation below shows the process of the regression as data is measured, point by point, and how the posteriors are updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90ff7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:44:33.710290Z",
     "start_time": "2023-02-03T18:44:33.687349Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = animate_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err, truth=w_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d426a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:44:49.420911Z",
     "start_time": "2023-02-03T18:44:49.419202Z"
    }
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92d598",
   "metadata": {},
   "source": [
    "The animation below shows the posterior distribution and some of its samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990b8cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:44:55.560410Z",
     "start_time": "2023-02-03T18:44:55.530565Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "post = parametric_bayesian_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err)\n",
    "anim = animate_distribution(x, post, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c938c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:45:08.634010Z",
     "start_time": "2023-02-03T18:45:08.631860Z"
    }
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d78ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:45:09.378474Z",
     "start_time": "2023-02-03T18:45:09.374879Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Made up data\n",
    "w_truth = (1, 2, -0.2, 0.02)\n",
    "feat_func_truth = _partial(feature_func_polynomial, num_features=len(w_truth))\n",
    "num_data = 10\n",
    "sigma_err = 3\n",
    "x_data = np.arange(-6, 6) + 0.5\n",
    "y_data = feat_func_truth(x_data) @ w_truth\n",
    "y_data += np.random.randn(*x_data.shape)*sigma_err\n",
    "\n",
    "# points for inference and truth values:\n",
    "num_pts = 100\n",
    "x = np.linspace(-6, 6, num_pts)\n",
    "truth = (x, feat_func_truth(x)@w_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0623a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:45:10.198235Z",
     "start_time": "2023-02-03T18:45:10.193294Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = _partial(feature_func_polynomial, num_features=4)\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = 3*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce25482",
   "metadata": {},
   "source": [
    "Notice that the formalism defined above can readly be applied to any model whose basis functions are fixed. For example, in the case of a polynomial we have:\n",
    "\n",
    "$$f(x) = \\omega_0 + \\omega_1x + ... + \\omega_px^p = \\vect{\\phi}^T\\vect{\\omega}$$\n",
    "\n",
    "where now $\\vect{\\omega}, \\vect{\\phi} \\in \\mathbb{R}^p$ with $\\phi = (1, x, ..., x^p)$ and the same equations above hold.\n",
    "\n",
    "So that we have the prior on $\\vect{\\omega}$ being a $p$-dimensional multivariate gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b6a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:45:27.352705Z",
     "start_time": "2023-02-03T18:45:27.328118Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = animate_distribution(x, prior, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e29e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:45:47.652995Z",
     "start_time": "2023-02-03T18:45:47.650366Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb4981",
   "metadata": {},
   "source": [
    "The regression evolution as new data comes in is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951393ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:45:49.556510Z",
     "start_time": "2023-02-03T18:45:49.532058Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim = animate_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err, truth=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448d0d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:46:06.153068Z",
     "start_time": "2023-02-03T18:46:06.151019Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b9d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:46:07.260585Z",
     "start_time": "2023-02-03T18:46:07.255179Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = _partial(feature_func_sines, num_freqs=6, L=6)\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = 10*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9c35d",
   "metadata": {},
   "source": [
    "The same idea also applies when the basis functions are not polynomials. For example, in case they are sines and cossines:\n",
    "\n",
    "$$\\vect{\\phi} = \\left(1,\n",
    "    \\sin\\left(\\frac{2\\pi x}{L}\\right), ..., \\sin\\left(\\frac{2\\pi px}{L}\\right),\n",
    "    \\cos\\left(\\frac{2\\pi x}{L}\\right), ..., \\cos\\left(\\frac{2\\pi px}{L}\\right)\\right)^T$$\n",
    "\n",
    "where $L\\in\\mathbb{R}$ is a constant.\n",
    "\n",
    "We have the following prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bf362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:46:18.188661Z",
     "start_time": "2023-02-03T18:46:18.159395Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim = animate_distribution(x, prior, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aadb08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:46:22.917519Z",
     "start_time": "2023-02-03T18:46:22.915660Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f4220",
   "metadata": {},
   "source": [
    "The regression process is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012115bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:46:24.064368Z",
     "start_time": "2023-02-03T18:46:24.030406Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = animate_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err, truth=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63f199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:46:36.021839Z",
     "start_time": "2023-02-03T18:46:36.019783Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67b972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:46:37.039620Z",
     "start_time": "2023-02-03T18:46:37.036305Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = _partial(feature_func_steps, step_range=6)\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = 10*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef6783",
   "metadata": {},
   "source": [
    "Discontinue functions are also an option. For example, in case they step functions:\n",
    "\n",
    "$$\\vect{\\phi} = (\\Theta(x-a_0),..., \\Theta(x-a_p))^T, \\,\\, \\mathrm{with}\\,\\,\n",
    "\\Theta(x) = \\left\\{\\begin{matrix}-1 & x<0 \\\\ 0 & x = 0 \\\\ 1 & x>0\\end{matrix}\\right.$$\n",
    "\n",
    "where $a_i\\in\\mathbb{R}$ for $i\\in[0,p]\\}$ are constants.\n",
    "\n",
    "We have the following prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2d80a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:46:55.996172Z",
     "start_time": "2023-02-03T18:46:55.968330Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim = animate_distribution(x, prior, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b766f6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:47:11.762087Z",
     "start_time": "2023-02-03T18:47:11.759845Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f572c6e",
   "metadata": {},
   "source": [
    "And the regression looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa48c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:47:13.256369Z",
     "start_time": "2023-02-03T18:47:13.205988Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = animate_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err, truth=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552de914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:47:26.686694Z",
     "start_time": "2023-02-03T18:47:26.684582Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dda0ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:47:27.441203Z",
     "start_time": "2023-02-03T18:47:27.431680Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = _partial(feature_func_gauss, mean_range=6)\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = 40*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a6f8d",
   "metadata": {},
   "source": [
    "An interesting particular case of this formalism comes when the basis functions are equally spaced gaussian functions with the same variance:\n",
    "\n",
    "$$\\vect{\\phi} = (\n",
    "\\exp\\left(-\\frac{(x-a_0)^2}{2\\sigma}\\right),\n",
    "...,\n",
    "\\exp\\left(-\\frac{(x-a_p)^2}{2\\sigma}\\right),$$\n",
    "\n",
    "where $\\sigma\\in\\mathbb{R}$ and $a_i\\in\\mathbb{R}$ for $i\\in[0,p]$ are both constants\n",
    "\n",
    "Note from the prior how the samples are very smooth functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1d282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:47:40.422923Z",
     "start_time": "2023-02-03T18:47:40.376067Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = animate_distribution(x, prior, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09313e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:47:53.751083Z",
     "start_time": "2023-02-03T18:47:53.749025Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee1e9e",
   "metadata": {},
   "source": [
    "Another interesting feature of this set of basis functions is that the posterior tends towards the prior away from the data, since all basis functions go to zero at $\\pm\\infty$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908383c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:47:55.264781Z",
     "start_time": "2023-02-03T18:47:55.237419Z"
    },
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = animate_regression(\n",
    "    x, x_data[2:-2], y_data[2:-2], prior, feat_func, sigma_err, truth=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fab318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T18:48:42.628713Z",
     "start_time": "2023-02-03T18:48:42.625158Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180608f3",
   "metadata": {},
   "source": [
    "To end this section on parametric fitting it is worth writting the posterior on $f(x)$ in a different manner. First recall that the posterior on $f(x)$ is given by:\n",
    "\n",
    "$$ f(x_*) \\sim \\mathcal{N}\\left(\\vect{\\phi_*}^T\\vect{\\bar{\\omega}_p},\\,\\, \\vect{\\phi_*}^T\\vect{\\Sigma_p}\\vect{\\phi_*}\\right)\n",
    "\\quad \\mathrm{with} \\quad\n",
    "\\vect{\\bar{\\omega}_p}=\\vect{\\bar{\\omega}} + \\frac{1}{\\sigma^2}\\vect{\\Sigma_p}\\vect{\\Phi}\\vect{y}\n",
    "\\quad \\mathrm{and} \\quad\n",
    "\\vect{\\Sigma_p}=\\left(\\sigma^{-2}\\vect{\\Phi}\\vect{\\Phi}^T + \\vect{\\Sigma}^{-1}\\right)^{-1}$$\n",
    "\n",
    "where we introduced $x_*$ to denote the coordinate where we want to predict $f(x)$ and $\\vect{\\phi_*}=\\vect{\\phi}(x_*)$\n",
    "Using an matrix identity, we can rewrite \n",
    "\n",
    "$$\n",
    "\\vect{\\phi_*}^T\\vect{\\bar{\\omega}_p} = \n",
    "\\vect{\\phi_*}^T\\vect{\\bar{\\omega}} + \\frac{1}{\\sigma^2}\\vect{\\phi_*}^T\\vect{\\Sigma_p}\\vect{\\Phi}(\\vect{y}-\\vect{\\Phi}^T\\vect{\\bar{\\omega}}) =\n",
    "\\bar{f}(x_*) + K(x_*, \\vect{x})\\left(K(\\vect{x}, \\vect{x}) + \\sigma^2\\vect{I}\\right)^{-1}(\\vect{y}-\\bar{f}(\\vect{x}))$$\n",
    "$$\n",
    "\\vect{\\phi_*}^T\\vect{\\Sigma_p}\\vect{\\phi_*} =\n",
    "\\vect{\\phi_*}^T\\left(\\sigma^{-2}\\vect{\\Phi}\\vect{\\Phi}^T + \\vect{\\Sigma}^{-1}\\right)^{-1}\\vect{\\phi_*} =\n",
    "K(x_*, x_*) - K(x_*,\\vect{x})\\left(K(\\vect{x}, \\vect{x}) + \\sigma^2\\vect{I}\\right)^{-1}K(x_*,\\vect{x})^T\n",
    "$$ \n",
    "\n",
    "where we introduced the prior mean of $f(x)$ calculated at $x_*$ as $\\bar{f}(x_*) = \\vect{\\phi_*}^T\\vect{\\bar{\\omega}}$ and the kernel function $K(x_i, x_j) = \\vect{\\phi(x_i)}^T\\vect{\\Sigma}\\vect{\\phi}(x_j)$, that calculates the correlation between different points $x_i$ and $x_j$.\n",
    "\n",
    "To summarize, the posterior becomes:\n",
    "\n",
    "$$f(x_*) \\sim \\mathcal{N}\\left(\n",
    "\\bar{f}(x_*) + K(x_*, \\vect{x})\\left(K(\\vect{x}, \\vect{x}) + \\sigma^2\\vect{I}\\right)^{-1}(\\vect{y}-\\bar{f}(\\vect{x})),\\,\\, \n",
    "K(x_*, x_*) - K(x_*,\\vect{x})\\left(K(\\vect{x}, \\vect{x}) + \\sigma^2\\vect{I}\\right)^{-1}K(x_*,\\vect{x})^T\n",
    "\\right)$$\n",
    "\n",
    "\n",
    "Note that written in this form, the posterior on $f(x)$ does not depend explicitly on how we parametized $f(x)$ in the first place. It only depends on how the the points where the inference is to be made, $x_*$ are correlated to themselves and to the data points, $\\vect{x}$, via the kernel function $K$.\n",
    "\n",
    "This fact gives us a hint on how to perform a non-parametric regression, being the core idea behing gaussian processes.\n",
    "\n",
    "We will see in the next section that all examples of parametric fitting shown here in the limit of an infinite number of basis functions for special cases of a gaussian process with different kernel functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471bbab",
   "metadata": {},
   "source": [
    "# Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993a8bb",
   "metadata": {},
   "source": [
    "With all the arguments presented above the definition of a gaussian process is very natural.\n",
    "\n",
    "We say that a function \n",
    "\n",
    "$$f(\\vect{x}): \\mathbb{R}^N \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "is described by a gaussian process:\n",
    "\n",
    "$$f(\\vect{x}) \\sim \\mathcal{GP}(m(\\vect{x}), K(\\vect{x}, \\vect{x'}))$$\n",
    "\n",
    "where $m(\\vect{x})$ is the prior mean function and $K(\\vect{x}, \\vect{x'})$ is the kernel function, if for any finite set \n",
    "\n",
    "$$\\mathcal{D}=\\{(\\vect{x_i}, f(\\vect{x_i})) | \\,\\, i\\in[1,p]\\} = (\\vect{X}, \\vect{f}(\\vect{X}))$$\n",
    "\n",
    "the values of $f(\\vect{x_i})$ are distributed according to a multivariate gaussian distribution:\n",
    "\n",
    "$$\\vect{f}(\\vect{X})\\sim\\mathcal{N}(\\vect{m}(\\vect{X}), \\vect{K}(\\vect{X}, \\vect{X}))).$$\n",
    "\n",
    "In the equations above we introduced the notations:\n",
    "\n",
    "$$\\vect{X}=(\\vect{x_1},..., \\vect{x_p}) \\in \\mathbb{R}^N\\times\\mathbb{R}^p$$\n",
    "\n",
    "$$\\vect{f}(\\vect{X})=(f(\\vect{x_1}),..., f(\\vect{x_p}))^T \\in \\mathbb{R}^p$$\n",
    "\n",
    "$$\\vect{m}(\\vect{X})=(m(\\vect{x_1}),..., m(\\vect{x_p}))^T \\in \\mathbb{R}^p$$\n",
    "\n",
    "$$\\vect{K}(\\vect{X}, \\vect{X}))= \n",
    "\\begin{pmatrix}\n",
    "K(\\vect{x_1}, \\vect{x_1}) & \\cdot & K(\\vect{x_1}, \\vect{x_p})\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "K(\\vect{x_p}, \\vect{x_1}) & \\cdot & K(\\vect{x_p}, \\vect{x_p})\\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^p\\times\\mathbb{R}^p$$\n",
    "\n",
    "to make them compatible with a matrix notation.\n",
    "\n",
    "$$\\begin{pmatrix}\\vect{f} \\\\ \\vect{f_*} \\end{pmatrix} \\sim \\mathcal{N}\\left(\n",
    "\\begin{pmatrix}\\vect{m} \\\\ \\vect{m_*}\\end{pmatrix},\\,\\,\n",
    "\\begin{pmatrix}\\vect{K} & \\vect{K_*}^T \\\\ \\vect{K_*} & \\vect{K_{**}} \\end{pmatrix}\n",
    "\\right)$$\n",
    "\n",
    "where we introduced a short notation \n",
    "$$\\vect{m}=\\vect{m}(\\vect{X}), \\quad \\vect{f}=\\vect{m}(\\vect{X}),$$ \n",
    "\n",
    "$$\\vect{m_*}=\\vect{m}(\\vect{X_*}),\\quad \\vect{f_*}=\\vect{m}(\\vect{X_*}),$$ \n",
    "\n",
    "$$\\vect{K}=\\vect{K}(\\vect{X}, \\vect{X}), \\quad \\vect{K_*}=\\vect{K}(\\vect{X_*}, \\vect{X})=\\vect{K}(\\vect{X}, \\vect{X_*})^T,\\,\\, \\mathrm{and} \\quad \\vect{K_{**}}=\\vect{K}(\\vect{X_*}, \\vect{X_*}).$$\n",
    "\n",
    "assuming we have noisy measurements $\\vect{y}$ of $\\vect{f}(\\vect{X})$:\n",
    "\n",
    "$$\\vect{y} = \\vect{f} + \\vect{\\varepsilon}$$\n",
    "\n",
    "$$\\begin{pmatrix}\\vect{y} \\\\ \\vect{f_*} \\end{pmatrix} \\sim \\mathcal{N}\\left(\n",
    "\\begin{pmatrix}\\vect{m} \\\\ \\vect{m_*}\\end{pmatrix},\\,\\,\n",
    "\\begin{pmatrix}\\vect{K} + \\sigma^2\\vect{I} & \\vect{K_*}^T \\\\ \\vect{K_*} & \\vect{K_{**}} \\end{pmatrix}\n",
    "\\right)$$\n",
    "\n",
    "$$P(\\vect{f_*}|\\vect{X}, \\vect{y}, \\vect{X^*}) = \\mathcal{N}\\left(\n",
    "\\vect{m_*} + \\vect{K_*}(\\vect{K}+\\sigma^2\\vect{I})^{-1})(\\vect{y}- \\vect{m}),\\,\\,\n",
    "\\vect{K_{**}} - \\vect{K_*}(\\vect{K}+\\sigma^2\\vect{I})^{-1})\\vect{K_*}^T)\n",
    "\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408d17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
