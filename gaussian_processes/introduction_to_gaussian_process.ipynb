{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23a876d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:51:50.253220Z",
     "start_time": "2023-02-15T16:51:49.613674Z"
    },
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from functools import partial as _partial\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as scyopt\n",
    "import scipy.stats as scystat\n",
    "from sklearn.datasets import make_regression as _create_data\n",
    "# import gpy\n",
    "import matplotlib.pyplot as mplt\n",
    "import ipywidgets\n",
    "\n",
    "import gp_lib as lib\n",
    "\n",
    "mplt.rcParams.update({\n",
    "    'font.size':10, 'axes.grid': True, 'grid.alpha': 0.5,\n",
    "    'grid.linestyle': '--', 'grid.linewidth': 1, 'lines.linewidth': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4764f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2aeeab",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{{#1}}}}}$\n",
    "The multivariate gaussian is the extension of the gaussian distribution to higher dimensions. It can be used to model the joint probability distribution of $N$ random variables $y_i$, which may be thought of as a vector in $\\vect{y}\\in \\mathbb{R}^N$, : \n",
    "\n",
    "$$\\vect{y} \\sim \\mathcal{N}(\\vect{\\mu}, \\vect{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^N|\\vect{\\Sigma}|}}\\exp\\left(-\\frac12(\\vect{y}-\\vect{\\mu})^T\\vect{\\Sigma}^{-1}(\\vect{y}-\\vect{\\mu})\\right)$$\n",
    "\n",
    "This distribution is completely characterized by the vector mean $\\vect{\\mu}\\in\\mathbb{R}^N$ and the covariance matrix $\\vect{\\Sigma}\\in\\mathbb{R}^N\\times\\mathbb{R}^N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb7215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:26:28.800415Z",
     "start_time": "2023-02-05T01:26:28.637031Z"
    },
    "hidden": true,
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = mplt.subplots(1, 1, figsize=(5, 3))\n",
    "ax.set_ylabel('Y1')\n",
    "ax.set_xlabel('Y0')\n",
    "ax.grid(False)\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y1 = np.linspace(-2, 2, 200)\n",
    "y1, y2 = np.meshgrid(y1, y1)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "mean = np.array([0.0, 0.0])\n",
    "pos -= mean\n",
    "\n",
    "surf = ax.pcolormesh(y1, y2, y2)\n",
    "fig.colorbar(surf, label='PDF')\n",
    "\n",
    "@ipywidgets.interact\n",
    "def plot_gauss(correlation=(-0.99, 0.99, 0.01)):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    covi = np.linalg.inv(cov)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    \n",
    "    z = np.exp(-1/2 * np.sum(pos @ covi * pos, axis=-1))\n",
    "    z *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov))\n",
    "\n",
    "    surf.set_array(z)\n",
    "    surf.set_clim([z.min(), z.max()])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d83bc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Gaussian distributions are closed under important operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c90fe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Marginalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185869b1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Marginalization is the process of integrating out some variables of the gaussian distribution and only looking at some of them. \n",
    "\n",
    "Consider the join distribution of variables $\\vect{x}$ and $\\vect{y}$:\n",
    "\n",
    "$$\\begin{pmatrix}\\vect{x} \\\\ \\vect{y}\\end{pmatrix} \\sim\n",
    "\\mathcal{N}\\left(\n",
    "    \\begin{pmatrix}\\vect{\\mu_x} \\\\ \\vect{\\mu_y}\\end{pmatrix},\n",
    "    \\begin{pmatrix}\\vect{\\Sigma_{xx}} & \\vect{\\Sigma_{xy}}\\\\ \\vect{\\Sigma_{xy}}^T & \\vect{\\Sigma_{yy}}\\end{pmatrix}\\right) $$\n",
    "\n",
    "If we are interested only on variable $\\vect{x}$ we can integrate out variable $\\vect{y}$:\n",
    "\n",
    "$$\\vect{x} \\sim \\int\\,\\mathrm{d}\\vect{y}P(\\vect{x},\\vect{y}) = \\mathcal{N}\\left(\\vect{\\mu_x}, \\vect{\\Sigma_{xx}}\\right)$$\n",
    "\n",
    "where we notice that not only the result is gaussian, but that all the correlation terms between $\\vect{x}$ and $\\vect{y}$ does not appear on the final result. This means that we could have indefinetly many gaussian-distributed correlated variables, but if we are only interested in a few of them, we don't need to care to all the other correlation terms.\n",
    "\n",
    "We can see this more clearly in the following 2D example, where we notice that varying the correlation does not change the dixtribution of $x$ on the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955b39c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:26:35.644742Z",
     "start_time": "2023-02-05T01:26:35.508884Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('x')\n",
    "ax.set_xlabel('y')\n",
    "ax.grid(False)\n",
    "ay.set_xlabel('x')\n",
    "ay.set_ylabel('PDF(x)')\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y = np.linspace(-4, 4, 200)\n",
    "y1, y2 = np.meshgrid(y, y)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "mean = np.array([0.0, 0.0])\n",
    "pos -= mean\n",
    "\n",
    "surf = ax.pcolormesh(y1, y2, y2)\n",
    "lin, = ay.plot(y, y)\n",
    "fig.colorbar(surf, label='PDF')\n",
    "\n",
    "@ipywidgets.interact\n",
    "def plot_gauss(correlation=(-0.99, 0.99, 0.01)):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    covi = np.linalg.inv(cov)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    \n",
    "    pdf = np.exp(-1/2 * np.sum(pos @ covi * pos, axis=-1))\n",
    "    pdf *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov))\n",
    "    \n",
    "    pdfx = np.trapz(pdf, axis=0)\n",
    "    \n",
    "    lin.set_data(y, pdfx)\n",
    "    ay.relim()\n",
    "    ay.autoscale_view()\n",
    "\n",
    "    surf.set_array(pdf)\n",
    "    surf.set_clim([pdf.min(), pdf.max()])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477cb71",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Conditioning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d58e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Conditioning is the process of fixing a subset of the random variables and looking at the distribution of the other variables.\n",
    "\n",
    "Consider the same setup defined above.\n",
    "\n",
    "$$\\begin{pmatrix}\\vect{x} \\\\ \\vect{y}\\end{pmatrix} \\sim\n",
    "\\mathcal{N}\\left(\n",
    "    \\begin{pmatrix}\\vect{\\mu_x} \\\\ \\vect{\\mu_y}\\end{pmatrix},\n",
    "    \\begin{pmatrix}\\vect{\\Sigma_{xx}} & \\vect{\\Sigma_{xy}}\\\\ \\vect{\\Sigma_{xy}}^T & \\vect{\\Sigma_{yy}}\\end{pmatrix}\\right) $$\n",
    "\n",
    "The conditional probability of $\\vect{x}$ when $\\vect{y}$ is fixed is given by:\n",
    "\n",
    "$$P(\\vect{x}|\\vect{y}=\\vect{a}) = \\mathcal{N}\\left(\\vect{\\mu_x} + \\vect{\\Sigma_{xy}}\\vect{\\Sigma_{yy}}^{-1}(\\vect{a}-\\vect{\\mu_y}),\\,\\, \\vect{\\Sigma_{xx}} - \\vect{\\Sigma_{xy}}\\vect{\\Sigma_{yy}}^{-1}\\vect{\\Sigma_{xy}}^T \\right)$$\n",
    "\n",
    "Note how in this case both, the mean and covariance, of the posterior on $\\vect{x}$ are changed.\n",
    "\n",
    "The example below show this fact for a 2D case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889438c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:26:41.656099Z",
     "start_time": "2023-02-05T01:26:41.501201Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('x')\n",
    "ax.set_xlabel('y')\n",
    "ax.grid(False)\n",
    "ay.set_xlabel('x')\n",
    "ay.set_ylabel('P(x|y)')\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y = np.linspace(-4, 4, 200)\n",
    "y1, y2 = np.meshgrid(y, y)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "mean = np.array([0.0, 0.0])\n",
    "pos -= mean\n",
    "\n",
    "surf = ax.pcolormesh(y1, y2, y2)\n",
    "hlin = ax.axline((0, 0), (1, 0), lw=1, ls='--', color='k')\n",
    "lin, = ay.plot(y, y)\n",
    "fig.colorbar(surf, label='PDF')\n",
    "\n",
    "@ipywidgets.interact\n",
    "def plot_gauss(correlation=(-0.99, 0.99, 0.01), y0=(-4, 4, 0.01)):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    covi = np.linalg.inv(cov)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    \n",
    "    pdf = np.exp(-1/2 * np.sum(pos @ covi * pos, axis=-1))\n",
    "    pdf *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov))\n",
    "    \n",
    "    sigx = cov[0, 0] - correlation**2/cov[1, 1]\n",
    "    mux = 0 + correlation/cov[1, 1]*(y0-0)\n",
    "    pdfx = np.exp(-1/2 * (y-mux)**2/sigx**2)\n",
    "    pdfx /= np.sqrt(2 * np.pi * sigx**2)\n",
    "    lin.set_data(y, pdfx)\n",
    "    ay.relim()\n",
    "    ay.autoscale_view()\n",
    "    \n",
    "    global hlin\n",
    "    hlin.remove()\n",
    "    hlin = ax.axline((0, y0), (1, y0), lw=1, ls='--', color='k')\n",
    "    surf.set_array(pdf)\n",
    "    surf.set_clim([pdf.min(), pdf.max()])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddd5de",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Affine Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183bcc1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An affine transformation of a random variable $\\vect{x}\\in\\mathbb{R}^N$ into another random variable $\\vect{y}\\in\\mathbb{R}^M$ is defined by:\n",
    "\n",
    "$$ \\vect{y} = \\vect{c} + \\vect{B}\\vect{x}$$\n",
    "\n",
    "where $\\vect{c} \\in \\mathbb{R}^M$ and $\\vect{B}\\in\\mathbb{R}^M\\times\\mathbb{R}^N$ are constants.\n",
    "\n",
    "Given that $\\vect{x}$ is distributed normally, $\\vect{x}\\sim\\mathcal{N}(\\vect{\\mu_x},\\, \\vect{\\Sigma})$, then $\\vect{y}$ will also be distributed normally:\n",
    "\n",
    "$\\vect{y}\\sim\\mathcal{N}(\\vect{c} + \\vect{B}\\vect{\\mu_x},\\, \\vect{B}\\vect{\\Sigma}\\vect{B}^T)$\n",
    "\n",
    "\n",
    "As an example, let's look to the 2D case below, where $\\vect{B}=(a, b)$, with $a,b\\in\\mathbb{R}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d095c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:26:49.254394Z",
     "start_time": "2023-02-05T01:26:49.005417Z"
    },
    "hidden": true,
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.set_xlabel('$x_2$')\n",
    "ax.grid(False)\n",
    "ay.set_xlabel('$y = c + ax_1 + bx_2$')\n",
    "ay.set_ylabel('P(y)')\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y = np.linspace(-4, 4, 200)\n",
    "y1, y2 = np.meshgrid(y, y)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "mean = np.array([0.0, 0.0])\n",
    "pos -= mean\n",
    "\n",
    "surf = ax.pcolormesh(y1, y2, y2)\n",
    "hlins = [\n",
    "    ax.axline((0, 0), slope=0, lw=1, ls='--', color='k')\n",
    "    for yi in np.linspace(-8, 8, 17)]\n",
    "lin, = ay.plot(y, y)\n",
    "fig.colorbar(surf, label='PDF')\n",
    "\n",
    "@ipywidgets.interact\n",
    "def plot_gauss(\n",
    "        correlation=(-0.99, 0.99, 0.01), a=(-10, 10, 0.01), b=(0.01, 1, 0.01),\n",
    "        c=(-4, 4, 0.01)):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    covi = np.linalg.inv(cov)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    \n",
    "    pdf = np.exp(-1/2 * np.sum(pos @ covi * pos, axis=-1))\n",
    "    pdf *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov))\n",
    "    \n",
    "    B = np.array([a, b])\n",
    "    sigy = B @ cov @ B.T\n",
    "    muy = c + B @ np.array([0, 0])\n",
    "    pdfy = np.exp(-1/2 * (y-muy)**2/sigy**2)\n",
    "    pdfy /= np.sqrt(2 * np.pi * sigy**2)\n",
    "    lin.set_data(y, pdfy)\n",
    "    ay.relim()\n",
    "    ay.autoscale_view()\n",
    "    \n",
    "    global hlins\n",
    "    _ = [h.remove() for h in hlins]\n",
    "    hlins = [\n",
    "        ax.axline((0, (yi-c)/b), slope=-a/b, lw=1, ls='--', color='w')\n",
    "        for yi in np.linspace(-8, 8, 17)]\n",
    "    surf.set_array(pdf)\n",
    "    surf.set_clim([pdf.min(), pdf.max()])\n",
    "    ax.set_xlim([-4, 4])\n",
    "    ax.set_ylim([-4, 4])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e92edb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Product of two distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182ed2f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The multivariate gaussian is also closed under multiplication. \n",
    "\n",
    "Given two normal distributions\n",
    "\n",
    "$$\\mathcal{N}(\\vect{\\mu_1},\\, \\vect{\\Sigma_1})\\quad\\mathcal{N}(\\vect{\\mu_2},\\, \\vect{\\Sigma_2}),$$\n",
    "\n",
    "the product between them is also a gaussian:\n",
    "\n",
    "$$\\mathcal{N}(\\vect{\\mu_3},\\, \\vect{\\Sigma_3})=\\mathcal{N}(\\vect{\\mu_1},\\, \\vect{\\Sigma_1})\\mathcal{N}(\\vect{\\mu_2},\\, \\vect{\\Sigma_2}),$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\vect{\\Sigma_3} = \\vect{\\Sigma_1}\\left(\\vect{\\Sigma_1} + \\vect{\\Sigma_2}\\right)^{-1}\\vect{\\Sigma_2}$$\n",
    "$$\\vect{\\mu_3} = \n",
    "\\vect{\\Sigma_2}\\left(\\vect{\\Sigma_1} + \\vect{\\Sigma_2}\\right)^{-1}\\vect{\\mu_1} + \n",
    "\\vect{\\Sigma_1}\\left(\\vect{\\Sigma_1} + \\vect{\\Sigma_2}\\right)^{-1}\\vect{\\mu_2}.$$\n",
    "\n",
    "Let's again look at a 2D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e217c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:26:54.721372Z",
     "start_time": "2023-02-05T01:26:54.505855Z"
    },
    "hidden": true,
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax, ay) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.set_xlabel('$x_2$')\n",
    "ax.grid(False)\n",
    "ay.set_xlabel('$x_1$')\n",
    "ay.set_ylabel('$x_2$')\n",
    "ay.grid(False)\n",
    "\n",
    "# Define the grid of x and y values\n",
    "y = np.linspace(-4, 4, 200)\n",
    "y1, y2 = np.meshgrid(y, y)\n",
    "pos = np.empty(y1.shape + (2,))\n",
    "pos[:, :, 0] = y1\n",
    "pos[:, :, 1] = y2\n",
    "\n",
    "surf1 = ax.pcolormesh(y1, y2, y2, alpha=0.3, cmap='jet')\n",
    "surf2 = ax.pcolormesh(y1, y2, y2, alpha=0.5, cmap='copper')\n",
    "surf3 = ay.pcolormesh(y1, y2, y2, alpha=1, cmap='jet')\n",
    "\n",
    "def plot_gauss(\n",
    "        corr1=0, mu1_1=0, mu1_2=0,\n",
    "        corr2=0, mu2_1=0, mu2_2=0,\n",
    "        calculated=False):\n",
    "    # Code done by chatgpt with a few modifications\n",
    "\n",
    "    # Define the covariance of the distribution\n",
    "    mu1 = np.array([mu1_1, mu1_2])\n",
    "    cov1 = np.eye(2)\n",
    "    cov1[0, 1] = corr1\n",
    "    cov1[1, 0] = corr1\n",
    "\n",
    "    mu2 = np.array([mu2_1, mu2_2])\n",
    "    cov2 = np.eye(2)\n",
    "    cov2[0, 1] = corr2\n",
    "    cov2[1, 0] = corr2\n",
    "\n",
    "    inv = np.linalg.inv(cov1 + cov2)\n",
    "    cov3 = cov1 @ inv @ cov2\n",
    "    mu3 = cov2 @ inv @ mu1 + cov1 @ inv @ mu2\n",
    "    \n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    cov1i = np.linalg.inv(cov1)\n",
    "    cov2i = np.linalg.inv(cov2)\n",
    "    cov3i = np.linalg.inv(cov3)\n",
    "\n",
    "    # Calculate the multivariate Gaussian distribution\n",
    "    pdf1 = np.exp(-1/2 * np.sum((pos-mu1) @ cov1i * (pos-mu1), axis=-1))\n",
    "    pdf1 *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov1))\n",
    "    \n",
    "    pdf2 = np.exp(-1/2 * np.sum((pos-mu2) @ cov2i * (pos-mu2), axis=-1))\n",
    "    pdf2 *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov2))\n",
    "    \n",
    "    if calculated:\n",
    "        pdf3 = np.exp(-1/2 * np.sum((pos-mu3) @ cov3i * (pos-mu3), axis=-1))\n",
    "        pdf3 *= 1/np.sqrt((2*np.pi)**2 * np.linalg.det(cov3))\n",
    "        surf3.set_cmap('copper')\n",
    "    else:\n",
    "        pdf3 = pdf1 * pdf2\n",
    "        surf3.set_cmap('jet')\n",
    "    \n",
    "    surf1.set_array(pdf1)\n",
    "    surf1.set_clim([pdf1.min(), pdf1.max()])\n",
    "\n",
    "    surf2.set_array(pdf2)\n",
    "    surf2.set_clim([pdf2.min(), pdf2.max()])\n",
    "\n",
    "    surf3.set_array(pdf3)\n",
    "    surf3.set_clim([pdf3.min(), pdf3.max()])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "wid = ipywidgets.interactive(\n",
    "    plot_gauss,\n",
    "    corr1=(-0.99, 0.99, 0.01), mu1_1=(-4, 4, 0.01), mu1_2=(-4, 4, 0.01),\n",
    "    corr2=(-0.99, 0.99, 0.01), mu2_1=(-4, 4, 0.01), mu2_2=(-4, 4, 0.01),\n",
    "    calculated=False)\n",
    "\n",
    "controls = ipywidgets.HBox(\n",
    "    wid.children[:-1],\n",
    "    layout=ipywidgets.Layout(flex_flow='row wrap'))\n",
    "output = wid.children[-1]\n",
    "display(ipywidgets.VBox([controls, output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c0faf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that all the properties described above can be calculated using very basic linear algebra, which implies that almost everything involving gaussians is very easy and fast to evaluate.\n",
    "\n",
    "These properties are the main reason gaussians are used everywhere in statistics, mainly in Bayesian Inference.\n",
    "\n",
    "What generally requires integration in a high dimensional space with other distributions can be accomplished with a few matrix multiplications when gaussians are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f572c3f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## A Different Way to Visualize Samples from Multivariate Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c96cdc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to build a more intuitive view of a gaussian process, let's first remember the multivariate gaussian distribution:\n",
    "\n",
    "$$\\vect{y} \\sim \\mathcal{N}(\\vect{\\mu}, \\vect{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^N|\\vect{\\Sigma}|}}\\exp\\left(-\\frac12(\\vect{y}-\\vect{\\mu})^T\\vect{\\Sigma}^{-1}(\\vect{y}-\\vect{\\mu})\\right)$$\n",
    "\n",
    "whose symbols meaning are the same as discussed previously.\n",
    "\n",
    "The usual way of visualizing this distribution is by plotting its PDF or drawing samples from the distribution and plotting one versus another.\n",
    "\n",
    "In the example below we will visualize a 2D gaussian differently. This approach will help us see the influence of the covariance between the random variables and extend the visualization into higher dimensions.\n",
    "\n",
    "Notice that when we increase the correlation between both variables the slope of lines on the right become smaller and the line approaches the horizontal line. On the other hand, when we make a high negative correlation the slopes become closer to $\\pm1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6952f23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:27:00.877407Z",
     "start_time": "2023-02-05T01:27:00.756572Z"
    },
    "hidden": true,
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ay, ax) = mplt.subplots(1, 2, figsize=(7, 3))\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('index')\n",
    "\n",
    "line, = ay.plot([0], [0], 'o')\n",
    "ay.set_ylabel(r'$y_1$')\n",
    "ay.set_xlabel(r'$y_0$')\n",
    "\n",
    "@ipywidgets.interact(correlation=(-0.999, 0.999, 0.001), n_samples=(1, 100, 1))\n",
    "def plot_gauss(correlation=0, n_samples=1):\n",
    "    [l.remove() for l in ax.lines]\n",
    "    x = np.linspace(0, 1, 2)\n",
    "    cov = np.eye(2)\n",
    "    cov[0, 1] = correlation\n",
    "    cov[1, 0] = correlation\n",
    "    mean = np.zeros(x.size)\n",
    "    data = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    ax.plot(x, data.T, lw=1)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    line.set_data(data[:, 0], data[:, 1])\n",
    "    ay.relim()\n",
    "    ay.autoscale_view()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58855364",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When we go to higher dimensions it is difficult to visualize the variables the same way we did on the left, but the graph on the right can be readly applied for an arbitrary number of dimensions.\n",
    "\n",
    "In the figure below we plot five samples of the $N$-dimensional vector of random variables as function of the coordinate index $i$. Note how the curves change as we increase the correlation between the several coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5b995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-09T19:59:58.444732Z",
     "start_time": "2023-02-09T19:59:58.099045Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "fig, ax = mplt.subplots(1, 1, figsize=(7, 3))\n",
    "ax.set_ylabel(r'$y$')\n",
    "ax.set_xlabel('index')\n",
    "\n",
    "@ipywidgets.interact(correlation=(1e-3, 0.999, 0.001), ndim=(2, 500, 1))\n",
    "def plot_gauss(correlation=1e-3, ndim=2):\n",
    "    ncurves=5\n",
    "    [l.remove() for l in ax.lines]\n",
    "    x = np.arange(ndim, dtype=float)/ndim\n",
    "    cov = lib.KernelFuncs.squared_exponential(x, x, leng=correlation)\n",
    "    mean = np.zeros(x.size)\n",
    "    data = np.random.multivariate_normal(mean, cov, size=ncurves)\n",
    "    ax.plot(data.T, lw=1)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ba508",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I was not completely honest in the graph above, because when we are in 2 dimensions there is only one way we can increase the correlation between the two variables. However when we go to higher dimensions, $N$, there are $N(N-1)/2$ free coefficients in the covariance matrix to define the covariance between each pair of coordinates. In the graph above I changed the covariance matrix in a very specific way:\n",
    "\n",
    "$$\\Sigma_{ij} = K(i, j) := \\exp\\left(-\\frac12\\left(\\frac{i-j}{Nl}\\right)^2\\right)$$\n",
    "\n",
    "where $K(i,j)$ is called kernel function and $l$ is the variable we change in the knob of the graphic.\n",
    "\n",
    "Note that the self-correlation, or the variance of every varible, is $K(i, i)=\\exp(0)=1$ and the correlation between variables decays as they are more apart in the index set.\n",
    "\n",
    "If I used another kernel we would have a very different figure. For example, consider the following covariance matrix:\n",
    "\n",
    "$$\\Sigma_{ij} = K(i, j) := \\exp\\left(-\\frac{|i-j|}{Nl}\\right)$$\n",
    "\n",
    "we get the following behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66d8ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:28:32.635874Z",
     "start_time": "2023-02-05T01:28:32.567890Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = mplt.subplots(1, 1, figsize=(7, 3))\n",
    "ax.set_ylabel(r'$y$')\n",
    "ax.set_xlabel('index')\n",
    "\n",
    "@ipywidgets.interact(correlation=(1e-3, 0.999, 0.001), ndim=(2, 500, 1))\n",
    "def plot_gauss(correlation=1e-3, ndim=2):\n",
    "    ncurves=5\n",
    "    [l.remove() for l in ax.lines]\n",
    "    x = np.arange(ndim, dtype=float)/ndim\n",
    "    cov = lib.KernelFuncs.exponential(x, x, leng=correlation)\n",
    "    mean = np.zeros(x.size)\n",
    "    data = np.random.multivariate_normal(mean, cov, size=ncurves)\n",
    "    ax.plot(data.T, lw=1)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c269d2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "which is continuous but not differentiable and is compatible with the brownian motion of a particle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdf6f3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bayes Theorem and Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182c99b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we have a physical quantity $y$ whose value we don't know but we think it is distributed normally with $\\sigma=3$. Since we don't know the value, we assume $\\mu=0$. The PDF of this variable is given by:\n",
    "\n",
    "$$P(y|\\mu=0,\\sigma=3) = \\mathcal{N}(0,3) = \\frac{1}{3\\sqrt{2\\pi}}\\exp\\left(-\\frac12\\frac{y^2}{3^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b0208c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:51:50.257864Z",
     "start_time": "2023-02-15T16:51:50.254764Z"
    },
    "hidden": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Code made by chatgpt with a few modifications\n",
    "\n",
    "# Define the prior mean and standard deviation\n",
    "mu_0 = 0\n",
    "sigma_0 = 3\n",
    "\n",
    "# Define a range of X values\n",
    "X = np.linspace(-20, 20, 10000)\n",
    "\n",
    "# Calculate the prior PDF\n",
    "prior_pdf = scystat.norm.pdf(X, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f9289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:29:00.299949Z",
     "start_time": "2023-02-05T01:29:00.242935Z"
    },
    "hidden": true,
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the prior PDF\n",
    "fig, ax = mplt.subplots(1, 1, figsize=(5, 3))\n",
    "ax.plot(X, prior_pdf, label='Prior')\n",
    "ax.set_xlabel('y')\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.legend(loc='best')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ccac3",
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "Now suppose we make a measurement $y_0$ of this quantity and the value is $5$. The measurement procedure has an uncertainty whose standard deviation is $1$. We could say that the result of the measurement also is a gaussian:\n",
    "\n",
    "$$P(y_0|y, \\mu_0=5, \\sigma_0=1) = \\mathcal{N}(5, 1) = \\frac{1}{1\\sqrt{2\\pi}}\\exp\\left(-\\frac12\\frac{(y_0-5)^2}{1^2}\\right)$$\n",
    "\n",
    "we can use Bayes Theorem to update our beliefs given this new information:\n",
    "\n",
    "$$P(y|y_0) = \\frac{P(y_0|y)P(y)}{P(y_0)}$$\n",
    "\n",
    "where :\n",
    " - $P(y|y_0)$ is called the posterior, because it represents our updated beliefs on $y$ after the measurement;\n",
    " - $P(y)=P(y|\\mu,\\sigma)$ is our prior assumption;\n",
    " - $P(y_0|y)$ called likelihood, which in our case is the measurement information;\n",
    " - $P(y_0)$ is the probability of having measured $y_0$ (marginal likelihood) which here we can consider just as a normalization constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fed84f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:51:50.338368Z",
     "start_time": "2023-02-15T16:51:50.258920Z"
    },
    "hidden": true,
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Define the measurement mean and standard deviation\n",
    "mu_x = [5, 1, 3, 2, 3.5]\n",
    "sigma_x = [1, 4, 1.5, 2, 0.5]\n",
    "frames = list(range(len(mu_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4914c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:30:12.414485Z",
     "start_time": "2023-02-05T01:30:12.354970Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.bayes(frames[:1], mu_x, sigma_x, X, prior_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06a0d8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If more data is measured our beliefs can be updated considering the previous posterior distribution as prior:\n",
    "\n",
    "$$P(y|\\{y_0,y_1\\}) = \\frac{P(y_1|y)P(y|y_0)}{P(y_1)} = \\frac{P(y_1|y)P(y_0|y)P(y)}{P(y_1)P(y_0)}$$\n",
    "\n",
    "This process can be repeated indefinetly considering new measurements are made:\n",
    "\n",
    "$$P(y|\\mathcal{D}) = \\frac{\\prod_\\mathcal{D}P(y_i|y)}{\\prod_\\mathcal{D}P(y_i)}P(y), \\quad \\mathcal{D}=\\{y_i| i=0..N\\} \\quad\\mathrm{and}\\quad y_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22d690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:30:21.324086Z",
     "start_time": "2023-02-05T01:30:21.255340Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.bayes(frames, mu_x, sigma_x, X, prior_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6906251",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is worth noticing that the final posterior does not depend on the order of the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc5c9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:30:26.263678Z",
     "start_time": "2023-02-05T01:30:26.194873Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim2 = lib.Animate.bayes(\n",
    "    np.random.permutation(frames), mu_x, sigma_x, X, prior_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76191bc1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Parametric Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b4678",
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "As an example let's see how we could use the framework defined above to perform parametric regression of some curves.\n",
    "\n",
    "First, let's start with the simple straight line model:\n",
    "\n",
    "$$ f(x) = \\omega_1 + \\omega_2x = \\vect{\\phi}(x)^T\\vect{\\omega}$$\n",
    "\n",
    "where $\\vect{\\omega} \\in \\mathbb{R}^2$ is the vector of random variables we want to know more about and $\\vect{\\phi}(x) = (1, x)^T$. When we look this way we see that the function is a random variable defined by the affine transformation $\\vect{\\phi}(x)$ of $\\vect{\\omega}$.\n",
    "\n",
    "Now lets assume a gaussian prior on $\\vect{\\omega}$:\n",
    "\n",
    "$$ \\vect{\\omega} \\sim \\mathcal{N}\\left(\\vect{\\bar{\\omega}}, \\vect{\\Sigma}\\right)$$\n",
    "\n",
    "Considering that f is just a linear combination of $\\vect{\\omega}$, the prior on $f(x)$ is given by:\n",
    "\n",
    "$$ f(x) \\sim \\mathcal{N}\\left(\\vect{\\phi}^T\\vect{\\bar{\\omega}},\\,\\, \\vect{\\phi}^T\\vect{\\Sigma}\\vect{\\phi}\\right)$$\n",
    "\n",
    "The figure below shows the general behavior of this prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a234ac0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:51:50.427124Z",
     "start_time": "2023-02-15T16:51:50.340490Z"
    },
    "hidden": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Made up data\n",
    "w_truth = (1, 2)\n",
    "feat_func_truth = _partial(\n",
    "    lib.FeatureFuncs.polynomial, num_features=len(w_truth))\n",
    "num_data = 10\n",
    "sigma_err = 1\n",
    "x_data = (np.random.rand(num_data)-0.5)*2 * 6\n",
    "y_data = feat_func_truth(x_data) @ w_truth\n",
    "y_data += np.random.randn(*x_data.shape)*sigma_err\n",
    "\n",
    "# points for inference and truth values:\n",
    "num_pts = 100\n",
    "x = np.linspace(-6, 6, num_pts)\n",
    "truth = (x, feat_func_truth(x)@w_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd7176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:30:35.448747Z",
     "start_time": "2023-02-05T01:30:35.420577Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = lib.FeatureFuncs.linear\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = 3*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ccb83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:30:37.679330Z",
     "start_time": "2023-02-05T01:30:37.642596Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_distribution(x, prior, feature_func=feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6454c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:30:41.072143Z",
     "start_time": "2023-02-05T01:30:41.067905Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a177e47",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, suppose that a total of $N$ measurements are performed, such that:\n",
    "\n",
    "$$\\mathcal{D} = \\{(x_i, y_i)| i\\in[1,N]\\} = (\\vect{x}, \\vect{y})$$\n",
    "\n",
    "where $y_i$ represent noisy observations of the underlying function $f(x_i)$:\n",
    "\n",
    "$$y_i = f(x_i) + \\varepsilon, \\quad\\mathrm{with}\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$$\n",
    "\n",
    "This way we se that the likelyhood of all the measurements is given by:\n",
    "\n",
    "$$P(\\vect{y}|\\vect{\\omega}, \\mathcal{D}) = \\prod_\\mathcal{D}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_i-\\vect{\\phi}(x_i)^T\\vect{\\omega})^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "Since we are considering all measurement error distributions are equally distributed (same $\\sigma^2$) and gaussian. We can simplify the equation above such that:\n",
    "\n",
    "$$P(\\vect{y}|\\vect{\\omega}, \\mathcal{D}) = \\frac{1}{(2\\pi)^{N/2}\\sigma^N}\\exp\\left(-\\frac{|\\vect{y}-\\vect{\\Phi}^T\\vect{\\omega}|^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "where we introduced the matrix $\\vect{\\Phi} := (\\phi(x_1),...\\phi(x_i),...\\phi(x_N))$.\n",
    "\n",
    "The posterior distribution can then be updated using Bayes theorem:\n",
    "\n",
    "$$P(\\vect{\\omega}|\\mathcal{D}) = \\frac{P(\\vect{y}|\\vect{x},\\vect{\\omega})P(\\vect{\\omega})}{P(\\vect{y}|\\vect{x})} \\propto\n",
    "\\exp\\left(-\\frac{1}{2\\sigma^2}\\left(\\vect{y}-\\vect{\\Phi}^T\\vect{\\omega}\\right)^T\\left(\\vect{y}-\\vect{\\Phi}^T\\vect{\\omega}\\right)\\right)\n",
    "\\exp\\left(-\\frac12(\\vect{\\omega}-\\vect{\\bar{\\omega}})^T\\vect{\\Sigma}^{-1}(\\vect{\\omega}-\\vect{\\bar{\\omega}})\\right)\n",
    "$$\n",
    "\n",
    "Since all distributions are gaussian, we can analytically compute the posterior by completing the squares on the equation above. After some math we see the posterior is also gaussian:\n",
    "\n",
    "$$P(\\vect{\\omega}|\\mathcal{D}) = \\mathcal{N}(\\vect{\\bar{\\omega}_p}, \\vect{\\Sigma_p}),\n",
    "\\quad \\mathrm{with} \\quad\n",
    "\\vect{\\bar{\\omega}_p}=\n",
    "\\vect{\\bar{\\omega}} + \\frac{1}{\\sigma^2}\\vect{\\Sigma_p}\\vect{\\Phi}(\\vect{y}-\\vect{\\Phi}^T\\vect{\\bar{\\omega}})\n",
    "\\quad \\mathrm{and} \\quad\n",
    "\\vect{\\Sigma_p}=\\left(\\sigma^{-2}\\vect{\\Phi}\\vect{\\Phi}^T + \\vect{\\Sigma}^{-1}\\right)^{-1}$$\n",
    "\n",
    "and, consequently, the posterior on $f(x)$ is also gaussian:\n",
    "\n",
    "$$ f(x) \\sim \\mathcal{N}\\left(\\vect{\\phi}^T\\vect{\\bar{\\omega}_p},\\,\\, \\vect{\\phi}^T\\vect{\\Sigma_p}\\vect{\\phi}\\right)$$\n",
    "\n",
    "The animation below shows the process of the regression as data is measured, point by point, and how the posteriors are updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90ff7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:30:44.516710Z",
     "start_time": "2023-02-05T01:30:44.478137Z"
    },
    "hidden": true,
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err, truth=w_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d426a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:30:48.062018Z",
     "start_time": "2023-02-05T01:30:48.058377Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92d598",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The animation below shows the posterior distribution and some of its samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990b8cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:30:59.427042Z",
     "start_time": "2023-02-05T01:30:59.393365Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "post = lib.Regressions.parametric_bayesian(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err)\n",
    "anim = lib.animate_distribution(x, post, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c938c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T01:31:02.739562Z",
     "start_time": "2023-02-05T01:31:02.736210Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82d78ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:51:50.547705Z",
     "start_time": "2023-02-15T16:51:50.432203Z"
    },
    "hidden": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Made up data\n",
    "num_data = 10\n",
    "sigma_err = 20\n",
    "x_data = np.linspace(-5.5, 5.5, num_data)\n",
    "x_data += np.random.randn(num_data)*0.2\n",
    "x_data[x_data<-6] = -5.9\n",
    "x_data[x_data>+6] = +5.9\n",
    "x_data = np.random.permutation(x_data)\n",
    "X = np.vstack([x_data, 2*np.ones(num_data)])\n",
    "# y_data = np.log(lib.TestFunctions.rosenbrock(X))\n",
    "y_data = lib.TestFunctions.himmelblau(X/1.2)\n",
    "y_data += np.random.randn(*x_data.shape)*sigma_err\n",
    "\n",
    "# points for inference and truth values:\n",
    "num_pts = 100\n",
    "x = np.linspace(-6, 6, num_pts)\n",
    "X = np.vstack([x, 2*np.ones(num_pts)])\n",
    "# fx = np.log(lib.TestFunctions.rosenbrock(X))\n",
    "fx = lib.TestFunctions.himmelblau(X/1.2)\n",
    "cov_amp = np.abs(fx).max()**2\n",
    "truth = (x, fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e43ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T19:45:33.713960Z",
     "start_time": "2023-02-10T19:45:33.711865Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = mplt.subplots(1, 1, figsize=(7, 4))\n",
    "\n",
    "# ax.plot(x, lib.TestFunctions.styblinski(X/1.5), label='styblinski')\n",
    "# ax.plot(x, lib.TestFunctions.rosenbrock(X/8), label='rosenbrock')\n",
    "# ax.plot(x, lib.TestFunctions.beale(X), label='beale')\n",
    "# ax.plot(x, lib.TestFunctions.himmelblau(X/1.2), label='himmelblau')\n",
    "# ax.plot(x, lib.TestFunctions.eggholder(X*10), label='eggholder')\n",
    "# ax.legend(loc='best')\n",
    "# fig.tight_layout()\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0623a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:00:35.316207Z",
     "start_time": "2023-02-10T20:00:35.312607Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = _partial(lib.FeatureFuncs.polynomial, num_features=5)\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = cov_amp*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce25482",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that the formalism defined above can readly be applied to any model whose basis functions are fixed. For example, in the case of a polynomial we have:\n",
    "\n",
    "$$f(x) = \\omega_0 + \\omega_1x + ... + \\omega_px^p = \\vect{\\phi}^T\\vect{\\omega}$$\n",
    "\n",
    "where now $\\vect{\\omega}, \\vect{\\phi} \\in \\mathbb{R}^p$ with $\\phi = (1, x, ..., x^p)$ and the same equations above hold.\n",
    "\n",
    "So that we have the prior on $\\vect{\\omega}$ being a $p$-dimensional multivariate gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b6a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T19:50:53.899450Z",
     "start_time": "2023-02-10T19:50:53.857301Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_distribution(x, prior, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e29e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T19:50:57.089845Z",
     "start_time": "2023-02-10T19:50:57.087593Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb4981",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The regression evolution as new data comes in is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951393ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:01:29.241996Z",
     "start_time": "2023-02-10T20:01:29.220325Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err, truth=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448d0d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:08:53.832473Z",
     "start_time": "2023-02-10T20:08:53.830308Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b9d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:08:55.094340Z",
     "start_time": "2023-02-10T20:08:55.089995Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = _partial(lib.FeatureFuncs.sines, num_freqs=6, L=6)\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = cov_amp*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9c35d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The same idea also applies when the basis functions are not polynomials. For example, in case they are sines and cossines:\n",
    "\n",
    "$$\\vect{\\phi} = \\left(1,\n",
    "    \\sin\\left(\\frac{2\\pi x}{L}\\right), ..., \\sin\\left(\\frac{2\\pi px}{L}\\right),\n",
    "    \\cos\\left(\\frac{2\\pi x}{L}\\right), ..., \\cos\\left(\\frac{2\\pi px}{L}\\right)\\right)^T$$\n",
    "\n",
    "where $L\\in\\mathbb{R}$ is a constant.\n",
    "\n",
    "We have the following prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bf362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T19:51:19.594714Z",
     "start_time": "2023-02-10T19:51:19.513147Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_distribution(x, prior, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aadb08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T19:51:23.196353Z",
     "start_time": "2023-02-10T19:51:23.193613Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f4220",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The regression process is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012115bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:08:59.100022Z",
     "start_time": "2023-02-10T20:08:59.057467Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err, truth=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63f199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:09:14.095587Z",
     "start_time": "2023-02-10T20:09:14.093507Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67b972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:09:14.636036Z",
     "start_time": "2023-02-10T20:09:14.625664Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = _partial(lib.FeatureFuncs.steps, step_range=6)\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = cov_amp*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef6783",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Discontinue functions are also an option. For example, in case they step functions:\n",
    "\n",
    "$$\\vect{\\phi} = (\\Theta(x-a_0),..., \\Theta(x-a_p))^T, \\,\\, \\mathrm{with}\\,\\,\n",
    "\\Theta(x) = \\left\\{\\begin{matrix}-1 & x<0 \\\\ 0 & x = 0 \\\\ 1 & x>0\\end{matrix}\\right.$$\n",
    "\n",
    "where $a_i\\in\\mathbb{R}$ for $i\\in[0,p]\\}$ are constants.\n",
    "\n",
    "We have the following prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2d80a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T19:51:49.477355Z",
     "start_time": "2023-02-10T19:51:49.431403Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_distribution(x, prior, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b766f6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T19:51:51.971714Z",
     "start_time": "2023-02-10T19:51:51.968212Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f572c6e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And the regression looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa48c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:09:16.543332Z",
     "start_time": "2023-02-10T20:09:16.508168Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err, truth=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552de914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:09:32.993288Z",
     "start_time": "2023-02-10T20:09:32.990130Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dda0ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:09:33.854007Z",
     "start_time": "2023-02-10T20:09:33.844365Z"
    },
    "hidden": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Prior on omega\n",
    "feat_func = _partial(lib.FeatureFuncs.gauss, mean_range=6, sigma=2)\n",
    "mu_prior = np.zeros(feat_func(np.zeros(1)).shape[1])\n",
    "sigma_prior = cov_amp*np.eye(mu_prior.size)\n",
    "prior = scystat.multivariate_normal(mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a6f8d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An interesting particular case of this formalism comes when the basis functions are equally spaced gaussian functions with the same variance:\n",
    "\n",
    "$$\\vect{\\phi} = (\n",
    "\\exp\\left(-\\frac{(x-a_0)^2}{2\\sigma}\\right),\n",
    "...,\n",
    "\\exp\\left(-\\frac{(x-a_p)^2}{2\\sigma}\\right),$$\n",
    "\n",
    "where $\\sigma\\in\\mathbb{R}$ and $a_i\\in\\mathbb{R}$ for $i\\in[0,p]$ are both constants\n",
    "\n",
    "Note from the prior how the samples are very smooth functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1d282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T19:53:04.851542Z",
     "start_time": "2023-02-10T19:53:04.821948Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_distribution(x, prior, feat_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09313e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T19:53:07.625224Z",
     "start_time": "2023-02-10T19:53:07.623097Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim.repeat=False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee1e9e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another interesting feature of this set of basis functions is that the posterior tends towards the prior away from the data, since all basis functions go to zero at $\\pm\\infty$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908383c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:09:37.006920Z",
     "start_time": "2023-02-10T20:09:36.986529Z"
    },
    "hidden": true,
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = lib.Animate.parametric_regression(\n",
    "    x, x_data, y_data, prior, feat_func, sigma_err, truth=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fab318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T20:10:10.141400Z",
     "start_time": "2023-02-10T20:10:10.139602Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "anim.repeat = False\n",
    "# anim.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180608f3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To end this section on parametric fitting it is worth writting the posterior on $f(x)$ in a different manner. First recall that the posterior on $f(x)$ is given by:\n",
    "\n",
    "$$ f(x_*) \\sim \\mathcal{N}\\left(\\vect{\\phi_*}^T\\vect{\\bar{\\omega}_p},\\,\\, \\vect{\\phi_*}^T\\vect{\\Sigma_p}\\vect{\\phi_*}\\right)\n",
    "\\quad \\mathrm{with} \\quad\n",
    "\\vect{\\bar{\\omega}_p}=\\vect{\\bar{\\omega}} + \\frac{1}{\\sigma^2}\\vect{\\Sigma_p}\\vect{\\Phi}\\vect{y}\n",
    "\\quad \\mathrm{and} \\quad\n",
    "\\vect{\\Sigma_p}=\\left(\\sigma^{-2}\\vect{\\Phi}\\vect{\\Phi}^T + \\vect{\\Sigma}^{-1}\\right)^{-1}$$\n",
    "\n",
    "where we introduced $x_*$ to denote the coordinate where we want to predict $f(x)$ and $\\vect{\\phi_*}=\\vect{\\phi}(x_*)$\n",
    "Using an matrix identity, we can rewrite \n",
    "\n",
    "$$\n",
    "\\vect{\\phi_*}^T\\vect{\\bar{\\omega}_p} = \n",
    "\\vect{\\phi_*}^T\\vect{\\bar{\\omega}} + \\frac{1}{\\sigma^2}\\vect{\\phi_*}^T\\vect{\\Sigma_p}\\vect{\\Phi}(\\vect{y}-\\vect{\\Phi}^T\\vect{\\bar{\\omega}}) =\n",
    "\\bar{f}(x_*) + K(x_*, \\vect{x})\\left(K(\\vect{x}, \\vect{x}) + \\sigma^2\\vect{I}\\right)^{-1}(\\vect{y}-\\bar{f}(\\vect{x}))$$\n",
    "$$\n",
    "\\vect{\\phi_*}^T\\vect{\\Sigma_p}\\vect{\\phi_*} =\n",
    "\\vect{\\phi_*}^T\\left(\\sigma^{-2}\\vect{\\Phi}\\vect{\\Phi}^T + \\vect{\\Sigma}^{-1}\\right)^{-1}\\vect{\\phi_*} =\n",
    "K(x_*, x_*) - K(x_*,\\vect{x})\\left(K(\\vect{x}, \\vect{x}) + \\sigma^2\\vect{I}\\right)^{-1}K(x_*,\\vect{x})^T\n",
    "$$ \n",
    "\n",
    "where we introduced the prior mean of $f(x)$ calculated at $x_*$ as $\\bar{f}(x_*) = \\vect{\\phi_*}^T\\vect{\\bar{\\omega}}$ and the kernel function $K(x_i, x_j) = \\vect{\\phi(x_i)}^T\\vect{\\Sigma}\\vect{\\phi}(x_j)$, that calculates the correlation between different points $x_i$ and $x_j$.\n",
    "\n",
    "To summarize, the posterior becomes:\n",
    "\n",
    "$$f(x_*) \\sim \\mathcal{N}\\left(\n",
    "\\bar{f}(x_*) + K(x_*, \\vect{x})\\left(K(\\vect{x}, \\vect{x}) + \\sigma^2\\vect{I}\\right)^{-1}(\\vect{y}-\\bar{f}(\\vect{x})),\\,\\, \n",
    "K(x_*, x_*) - K(x_*,\\vect{x})\\left(K(\\vect{x}, \\vect{x}) + \\sigma^2\\vect{I}\\right)^{-1}K(x_*,\\vect{x})^T\n",
    "\\right)$$\n",
    "\n",
    "\n",
    "Note that written in this form, the posterior on $f(x)$ does not depend explicitly on how we parametized $f(x)$ in the first place. It only depends on how the the points where the inference is to be made, $x_*$ are correlated to themselves and to the data points, $\\vect{x}$, via the kernel function $K$.\n",
    "\n",
    "This fact gives us a hint on how to perform a non-parametric regression, being the core idea behing gaussian processes.\n",
    "\n",
    "We will see in the next section that all examples of parametric fitting shown here in the limit of an infinite number of basis functions for special cases of a gaussian process with different kernel functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471bbab",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17215a4d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With all the arguments presented above the definition of a gaussian process is very natural.\n",
    "\n",
    "We say that a function \n",
    "\n",
    "$$f(\\vect{x}): \\mathbb{R}^N \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "is described by a gaussian process:\n",
    "\n",
    "$$f(\\vect{x}) \\sim \\mathcal{GP}(m(\\vect{x}), K(\\vect{x}, \\vect{x'}))$$\n",
    "\n",
    "where \n",
    "\n",
    "$$m(\\vect{x}): \\mathbb{R}^N \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "$$K(\\vect{x}, \\vect{x'}):\\mathbb{R}^N\\times\\mathbb{R}^N \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "are the prior mean function and the covariance function (kernel), if for any finite set \n",
    "\n",
    "$$\\mathcal{D}=\\{(\\vect{x_i}, f(\\vect{x_i})) | \\,\\, i\\in[1,p]\\} = (\\vect{X}, \\vect{f}(\\vect{X}))$$\n",
    "\n",
    "the values of $f(\\vect{x_i})$ are distributed according to a multivariate gaussian distribution:\n",
    "\n",
    "$$\\vect{f}(\\vect{X})\\sim\\mathcal{N}(\\vect{m}(\\vect{X}), \\vect{K}(\\vect{X}, \\vect{X}))).$$\n",
    "\n",
    "In the equations above we introduced the notations:\n",
    "\n",
    "$$\\vect{X}=(\\vect{x_1},..., \\vect{x_p}) \\in \\mathbb{R}^N\\times\\mathbb{R}^p$$\n",
    "\n",
    "$$\\vect{f}(\\vect{X})=(f(\\vect{x_1}),..., f(\\vect{x_p}))^T \\in \\mathbb{R}^p$$\n",
    "\n",
    "$$\\vect{m}(\\vect{X})=(m(\\vect{x_1}),..., m(\\vect{x_p}))^T \\in \\mathbb{R}^p$$\n",
    "\n",
    "$$\\vect{K}(\\vect{X}, \\vect{X})= \n",
    "\\begin{pmatrix}\n",
    "K(\\vect{x_1}, \\vect{x_1}) & \\cdot & K(\\vect{x_1}, \\vect{x_p})\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "K(\\vect{x_p}, \\vect{x_1}) & \\cdot & K(\\vect{x_p}, \\vect{x_p})\\\\\n",
    "\\end{pmatrix}\\in \\mathbb{R}^p\\times\\mathbb{R}^p$$\n",
    "\n",
    "to make them compatible with a matrix notation.\n",
    "\n",
    "The covariance function is a type of kernel that creates positive-semidefinite matrices. This means that:\n",
    "\n",
    "$$\\begin{pmatrix}\\vect{x_1}^T & \\cdots & \\vect{x_p}^T\\end{pmatrix}\n",
    "\\begin{pmatrix} K_{11} & \\cdots & K_{1p}\\\\\\vdots &\\ddots&\\vdots\\\\ K_{p1} &\\cdots& K_{pp}\\end{pmatrix}\n",
    "\\begin{pmatrix} \\vect{x_1} \\\\ \\vdots \\\\ \\vect{x_p} \\end{pmatrix} = \n",
    "\\sum_{i,j}^p K_{ij}\\vect{x_i}^T\\vect{x_j}(2-\\delta_{ij})\n",
    "\\ge 0$$\n",
    "\n",
    "$$\\forall \\,\\,\\vect{x_i} \\in \\mathbb{R}^N,\\,\\, i \\in [1, p]\\in \\mathbb{N},\\,\\, K_{ij}=K_{ji}=K(\\vect{x_i},\\vect{x_i})$$\n",
    "\n",
    "This means that not every kernel function is a valid covariance function.\n",
    "\n",
    "Note that the behavior of the gaussian process is very dependent on the choice of the covariance function.\n",
    "\n",
    "To make this influence more clear, let's check some widely used examples of covariance functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcd387",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Covariance Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21a4d5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Most commonly used kernels are invariance under translations, which means that:\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) = K(\\vect{x_1} + \\vect{d}, \\vect{x_2} + \\vect{d})$$.\n",
    "\n",
    "In this case we can define the kernel as\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) =g(z)\\quad \\mathrm{with}\\quad z = (\\vect{x_1}-\\vect{x_2})^T\\vect{\\Sigma}^{-1}(\\vect{x_1}-\\vect{x_2}) $$\n",
    "\n",
    "where $\\vect{\\Sigma}$ is any positive semi-definite matrix. \n",
    "\n",
    "Generally $\\vect{\\Sigma}$ is chosen to be diagonal\n",
    "\n",
    "$$\\vect{\\Sigma} = \\begin{pmatrix}l_1^2 & & \\\\ & \\ddots & \\\\ & & l_N^2\\end{pmatrix}$$.\n",
    "\n",
    "In this case, the terms $l_i, i\\in[1,N]$ have interpretation of length scale of each direction of $\\mathbb{R}^N$.\n",
    "\n",
    "If the all the diagonal terms are equal $l_1 = l_2 = \\cdots = l_p = l$, then the kernel is said also be isotropic:\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) =g\\left(\\frac{(\\vect{x_1}-\\vect{x_2})^T(\\vect{x_1}-\\vect{x_2})}{l^2}\\right) = g\\left(\\frac{r}{l}\\right)$$\n",
    "\n",
    "Where $r$ has the usual interpretation of radius.\n",
    "\n",
    "In the examples below we will define the kernels in this most simplified way, but keep in mind that we can recover the most general form in all of them simply replacing $r^2\\rightarrow z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba06375",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Squared Exponential or RBF (Radial Basis Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91aa98f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This covariance function is the most commonly used. The class of functions it encapsulates are smooth functions, which means they can be derived infinitely many times and all derivatives are continuous. The kernel function reads:\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) = \\sigma^2 \\exp\\left(-\\frac{r^2}{2 l^2}\\right).$$\n",
    "\n",
    "This kernel is the limmiting case of a parametric fitting using off-centered gaussians of same width.\n",
    "\n",
    "Let's see below a 2D example of this kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd218d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:31:58.081212Z",
     "start_time": "2023-02-15T16:31:58.015851Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 300)\n",
    "muf = np.zeros(x.shape)\n",
    "cov = lib.KernelFuncs.squared_exponential(x, x, leng=1, sigma=1)\n",
    "dist = scystat.multivariate_normal(muf, cov, allow_singular=True)\n",
    "anim = lib.Animate.gp_distribution(x, dist, 'RBF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71523dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:31:59.061990Z",
     "start_time": "2023-02-15T16:31:59.059729Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7031e37",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Rational Quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ec184",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While the squared exponential kernel has a single length scale, this kernel is the infinite sum of squared exponentials with varying length scales:\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) = \\sigma^2 \\left(1 + \\frac1\\alpha\\frac{r^2}{2 l^2}\\right)^{-\\alpha}.$$\n",
    "\n",
    "where $\\alpha$ is a parameter that controls the distribution of length scales in kernel.\n",
    "\n",
    "Note how in the limit $\\alpha\\rightarrow\\infty$, we recover the squared exponential kernel.\n",
    "\n",
    "Folow below a 2D example of this kernel. Note how the functions have varying length scales along the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc856b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:31:35.408336Z",
     "start_time": "2023-02-15T16:31:35.359992Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 300)\n",
    "muf = np.zeros(x.shape)\n",
    "cov = lib.KernelFuncs.rational_quadratic(x, x, leng=1, sigma=1)\n",
    "dist = scystat.multivariate_normal(muf, cov, allow_singular=True)\n",
    "anim = lib.Animate.gp_distribution(x, dist, 'Rational Quad.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12aa2ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:31:37.703217Z",
     "start_time": "2023-02-15T16:31:37.700624Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70518a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701f8a9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The class of functions it encapsulates are continuous but not differentiable functions. The kernel function reads:\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) = \\sigma^2 \\exp\\left(-\\frac{r}{l}\\right).$$\n",
    "\n",
    "The functions of this kernel are compatible with the time evolution of the velocity of a particle undergoing browniang motion.\n",
    "\n",
    "Let's see below a 2D example of this kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e4ddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:31:24.079680Z",
     "start_time": "2023-02-15T16:31:24.002823Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 300)\n",
    "muf = np.zeros(x.shape)\n",
    "cov = lib.KernelFuncs.exponential(x, x, leng=2, sigma=1)\n",
    "dist = scystat.multivariate_normal(muf, cov, allow_singular=True)\n",
    "anim = lib.Animate.gp_distribution(x, dist, 'Exponential')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35712ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:31:25.072391Z",
     "start_time": "2023-02-15T16:31:25.070597Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec60263",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sine Squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74df4f2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This kernel is very useful to model periodic systems:\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2\\left(\\frac{\\pi r}{\\lambda}\\right)}{l^2}\\right).$$\n",
    "\n",
    "where $\\lambda$ is the period of the correlation.\n",
    "\n",
    "Let's see below a 2D example of this kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0e66d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:31:00.202021Z",
     "start_time": "2023-02-15T16:31:00.126952Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 300)\n",
    "muf = np.zeros(x.shape)\n",
    "cov = lib.KernelFuncs.sine_squared(x, x, leng=1, sigma=1, period=3)\n",
    "dist = scystat.multivariate_normal(muf, cov, allow_singular=True)\n",
    "anim = lib.Animate.gp_distribution(x, dist, 'Sine Squared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a3e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:31:02.708123Z",
     "start_time": "2023-02-15T16:31:02.705511Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf4ebd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Matérn Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19245e0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Matérn Kernels actually are a class of kernels, controlled by the parameter $\\nu$:\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) = \\sigma^2\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\frac{\\sqrt{2\\nu}r}{l}\\right)^\\nu K_\\nu\\left(\\frac{\\sqrt{2\\nu}r}{l}\\right),$$\n",
    "\n",
    "where $\\Gamma(\\cdot)$ is the gamma function and $K_\\nu(\\cdot)$ is the modified Bessel function.\n",
    "\n",
    "Even tough the Matérn Kernel is a valid covariance function for all values of $\\nu$, some particular choices are very interesting:\n",
    " - $\\nu\\rightarrow\\infty$: squared exponential kernel;\n",
    " - $\\nu=1/2$: exponential kernel;\n",
    " - $\\nu=3/2$: we get a kernel which is continuous up to its first derivative. In this case the function reads:\n",
    "   $$K(\\vect{x_1}, \\vect{x_2}) = \\sigma^2\\left(1 + \\frac{\\sqrt{3}r}{l}\\right)\\exp\\left(-\\frac{\\sqrt{3}r}{l}\\right)$$\n",
    "   \n",
    "- $\\nu=5/2$: we get a kernel which is continuous up to its second derivative. In this case the function reads:\n",
    "   $$K(\\vect{x_1}, \\vect{x_2}) = \\sigma^2\\left(1 + \\frac{\\sqrt{5}r}{l} + \\frac{5r^2}{3l^2}\\right)\\exp\\left(-\\frac{\\sqrt{5}r}{l}\\right)$$\n",
    "\n",
    "\n",
    "Let's check below the case when $\\nu=3/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259d7f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:30:42.716369Z",
     "start_time": "2023-02-15T16:30:42.618362Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 300)\n",
    "muf = np.zeros(x.shape)\n",
    "cov = lib.KernelFuncs.matern3over2(x, x, leng=1, sigma=1)\n",
    "dist = scystat.multivariate_normal(muf, cov, allow_singular=True)\n",
    "anim = lib.Animate.gp_distribution(x, dist, 'Matérn 3/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c77a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:30:45.344295Z",
     "start_time": "2023-02-15T16:30:45.341407Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fdbbf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Parametric Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f190387",
   "metadata": {
    "hidden": true
   },
   "source": [
    "another example of a valid kernel is the one obtained by parametric fitting. Let's remember that we can parametrize a function in terms of a set of basis functions:\n",
    "\n",
    "$f(\\vect{x}) = \\vect{\\phi}(\\vect{x})^T \\vect{\\omega}, \\quad \\vect{\\phi}(\\vect{x}) = (b_1(\\vect{x}), \\cdots, b_p(\\vect{x}))^T$$\n",
    "\n",
    "and the parameters $\\vect{\\omega}$ have a prior $\\mathcal{N}(\\vect{0}, \\vect{\\Sigma_p})$.\n",
    "We can define the kernel:\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) = \\vect{\\phi}(\\vect{x_1})^T\\vect{\\Sigma_p}\\vect{\\phi}(\\vect{x_2})$$\n",
    "\n",
    "such that all the framework of parametric fitting can be translated into the gaussian process framework.\n",
    "\n",
    "One important example of this type of kernel is the polynomial kernel:\n",
    "\n",
    "$$K(\\vect{x_1}, \\vect{x_2}) = (\\sigma_0 + \\vect{x_1}^T\\vect{x_2})^p$$\n",
    "\n",
    "where $p$ is the order of the polynomial.\n",
    "\n",
    "lets see a 1D example of this type of kernel, using a polynomial of order 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555293b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:30:26.874880Z",
     "start_time": "2023-02-15T16:30:26.803301Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 300)\n",
    "muf = np.zeros(x.shape)\n",
    "feat_func = lib.FeatureFuncs.linear\n",
    "cov = lib.KernelFuncs.parametric_function(x, x, feat_func, np.eye(2))\n",
    "dist = scystat.multivariate_normal(muf, cov, allow_singular=True)\n",
    "anim = lib.Animate.gp_distribution(x, dist, 'Linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f4495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:22:02.195609Z",
     "start_time": "2023-02-15T16:22:02.193348Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b3ac8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Building New Kernels from Old ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f45a8f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we have to kernels that form valid covariance functions $K_1(\\vect{x_1}, \\vect{x_2})$ and $K_2(\\vect{x_1}, \\vect{x_2})$ then:\n",
    " - the sum of the kernels is also a kernel: $K(\\vect{x_1}, \\vect{x_2}) = K_1(\\vect{x_1}, \\vect{x_2}) + K_2(\\vect{x_1}, \\vect{x_2})$\n",
    " - the product of the kernels is also a kernel: $K(\\vect{x_1}, \\vect{x_2}) = K_1(\\vect{x_1}, \\vect{x_2})K_2(\\vect{x_1}, \\vect{x_2})$\n",
    " - The application of the kernels in orthogonal subspaces is also a kernel:\n",
    " $$\\vect{x} = \\vect{y}\\bigoplus\\vect{z},\\,\\, \\vect{y}\\in\\mathbb{R}^m,\\,\\, \\vect{z}\\in\\mathbb{R}^n, \\,\\, \\vect{x}\\in\\mathbb{R}^{m+n}\\quad K(\\vect{x_1}, \\vect{x_2}) = K_1(\\vect{y_1}, \\vect{y_2}) + K_2(\\vect{z_1}, \\vect{z_2})$$\n",
    " \n",
    "In the example below we summed the liner kernel with the squared exponential kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f45a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:33:35.864932Z",
     "start_time": "2023-02-15T16:33:35.800098Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 300)\n",
    "muf = np.zeros(x.shape)\n",
    "feat_func = lib.FeatureFuncs.linear\n",
    "cov = lib.KernelFuncs.parametric_function(x, x, feat_func, np.eye(2))\n",
    "cov += lib.KernelFuncs.squared_exponential(x, x, sigma=3, leng=1)\n",
    "dist = scystat.multivariate_normal(muf, cov, allow_singular=True)\n",
    "anim = lib.Animate.gp_distribution(x, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aa86d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:33:38.664597Z",
     "start_time": "2023-02-15T16:33:38.662706Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7deec",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Hyper-Parameters of Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1994f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not only the type of the covariance function, but also the value of its hyper-parameters changes the look of the sample functions of the prior, as can be seen in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35154167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:36:05.559743Z",
     "start_time": "2023-02-15T16:36:05.306727Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 500)\n",
    "muf = np.zeros(x.shape)\n",
    "x_, y_ = np.meshgrid(x, x)\n",
    "\n",
    "fig, (ax, ay) = mplt.subplots(\n",
    "    1, 2, figsize=(7, 3), sharex=True, width_ratios=[1, 3])\n",
    "\n",
    "ax.set_ylabel('x')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_aspect('equal')\n",
    "ay.set_xlabel('x')\n",
    "ay.set_ylabel(r'$y = f(x) + \\varepsilon$')\n",
    "\n",
    "def plot_gp(kernel='Exp Squared', leng=1, sigma=1, period=1):\n",
    "    if kernel == 'Exp Squared':\n",
    "        kern_func = lib.KernelFuncs.squared_exponential\n",
    "    elif kernel == 'Exponential':\n",
    "        kern_func = lib.KernelFuncs.exponential\n",
    "    elif kernel == 'Matérn 3/2':\n",
    "        kern_func = lib.KernelFuncs.matern3over2\n",
    "    elif kernel == 'Matérn 5/2':\n",
    "        kern_func = lib.KernelFuncs.matern5over2\n",
    "    elif kernel == 'Rational Quad':\n",
    "        kern_func = lib.KernelFuncs.rational_quadratic\n",
    "    elif kernel == 'Sine Squared':\n",
    "        kern_func = _partial(lib.KernelFuncs.sine_squared, period=period)\n",
    "        \n",
    "    cov = kern_func(x, x, leng=leng, sigma=sigma)\n",
    "    dist = scystat.multivariate_normal(muf, cov, allow_singular=True)\n",
    "    stdf = np.sqrt(np.diag(cov))\n",
    "\n",
    "    # Create rotating samples from the distribution:\n",
    "    # First define the levels we want:\n",
    "    lev_smpl = -2*np.log(np.logspace(-6, 0, 10)*0.995)\n",
    "    # Create normalized random vectors for rotation:\n",
    "    N = muf.size\n",
    "    nvec = lev_smpl.size\n",
    "    v = np.random.randn(N, nvec)\n",
    "    v /= np.linalg.norm(v, axis=0)\n",
    "    v *= lev_smpl[None, :]\n",
    "\n",
    "    # Create transformation matrix from v\n",
    "    chol_cov = np.linalg.cholesky(cov + 1e-10*np.eye(cov.shape[0]))\n",
    "    v = muf[:, None] + chol_cov @ v\n",
    "\n",
    "    ax.pcolormesh(x_, y_, cov)\n",
    "    ax.set_title(kernel)\n",
    "\n",
    "    ay.clear()\n",
    "\n",
    "    ay.plot(x, muf, label=r'$\\mathbb{E}(f(x))$')\n",
    "    ay.fill_between(\n",
    "        x, muf+1.96*stdf, muf-1.96*stdf, color='C0', alpha=0.2,\n",
    "        label='95% confidence')\n",
    "    lines = [\n",
    "        ay.plot(\n",
    "            x, xd, '--', color=f'C{i:d}', lw=1)[0]\n",
    "        for i, xd in enumerate(v.T, 1)]\n",
    "    lines[0].set_label('samples')\n",
    "\n",
    "    ay.legend(\n",
    "        loc='lower center', bbox_to_anchor=(0.5, 0), fontsize='small')\n",
    "    fig.tight_layout()\n",
    "\n",
    "kerns = [\n",
    "    'Exp Squared', 'Exponential', 'Matérn 3/2', 'Matérn 5/2', 'Rational Quad',\n",
    "    'Sine Squared', ]\n",
    "wid = ipywidgets.interactive(\n",
    "    plot_gp,\n",
    "    kernel=kerns, leng=(0.001, 4, 0.01), sigma=(0.001, 10, 0.01),\n",
    "    period=(0.001, 10, 0.01))\n",
    "\n",
    "controls = ipywidgets.HBox(\n",
    "    wid.children[:-1],\n",
    "    layout=ipywidgets.Layout(flex_flow='row wrap'))\n",
    "output = wid.children[-1]\n",
    "display(ipywidgets.VBox([controls, output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd91ac",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression with Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993a8bb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The idea is that the Gaussian Process describes a prior, which can be updated when new data is measured via Bayes Theorem. Let's check how this works below: suppose we perform $m$ noisy measurements of $f(\\vect{x})$ at locations\n",
    "\n",
    "$$\\vect{X} = (\\vect{x_1}, \\cdots, \\vect{x_m}),$$\n",
    "\n",
    "and want to know the posterior of $f(x)$ at another $p$ points\n",
    "\n",
    "$$\\vect{X_*} = (\\vect{x_{m+1}}, \\cdots, \\vect{x_{m+p}}),$$\n",
    "\n",
    "The prior joint distribution of $\\vect{f}(\\vect{X})$ and $\\vect{f}(\\vect{X_*})$ is given by:\n",
    "\n",
    "$$\\begin{pmatrix}\\vect{f} \\\\ \\vect{f_*} \\end{pmatrix} \\sim \\mathcal{N}\\left(\n",
    "\\begin{pmatrix}\\vect{m} \\\\ \\vect{m_*}\\end{pmatrix},\\,\\,\n",
    "\\begin{pmatrix}\\vect{K} & \\vect{K_*}^T \\\\ \\vect{K_*} & \\vect{K_{**}} \\end{pmatrix}\n",
    "\\right)$$\n",
    "\n",
    "where we introduced a short notation \n",
    "$$\\vect{m}=\\vect{m}(\\vect{X}), \\quad \\vect{f}=\\vect{m}(\\vect{X}),$$ \n",
    "\n",
    "$$\\vect{m_*}=\\vect{m}(\\vect{X_*}),\\quad \\vect{f_*}=\\vect{m}(\\vect{X_*}),$$ \n",
    "\n",
    "$$\\vect{K}=\\vect{K}(\\vect{X}, \\vect{X}), \\quad \\vect{K_*}=\\vect{K}(\\vect{X_*}, \\vect{X})=\\vect{K}(\\vect{X}, \\vect{X_*})^T,\\,\\, \\mathrm{and} \\quad \\vect{K_{**}}=\\vect{K}(\\vect{X_*}, \\vect{X_*}).$$\n",
    "\n",
    "If we assume the noisy measurements $\\vect{y}$ all have the same uncertainty $\\varepsilon_i\\sim\\mathcal{N}(0, \\sigma)$:\n",
    "\n",
    "$$\\vect{y} = \\vect{f} + \\vect{\\varepsilon}, \\quad \\mathrm{with}\\quad\n",
    "\\vect{\\varepsilon}=(\\varepsilon_1,\\cdots, \\varepsilon_m),$$\n",
    "\n",
    "the joint distribution of $\\vect{y}$ and $\\vect{f_*}$ is given by:\n",
    "\n",
    "$$\\begin{pmatrix}\\vect{y} \\\\ \\vect{f_*} \\end{pmatrix} \\sim \\mathcal{N}\\left(\n",
    "\\begin{pmatrix}\\vect{m} \\\\ \\vect{m_*}\\end{pmatrix},\\,\\,\n",
    "\\begin{pmatrix}\\vect{K} + \\sigma^2\\vect{I} & \\vect{K_*}^T \\\\ \\vect{K_*} & \\vect{K_{**}} \\end{pmatrix}\n",
    "\\right).$$\n",
    "\n",
    "This way, the posterior of $\\vect{f_*}$ can be calculated simply by taking the conditional probability below:\n",
    "\n",
    "$$P(\\vect{f_*}|\\vect{X}, \\vect{y}, \\vect{X^*}) = \\mathcal{N}\\left(\n",
    "\\vect{m_*} + \\vect{K_*}(\\vect{K}+\\sigma^2\\vect{I})^{-1})(\\vect{y}- \\vect{m}),\\,\\,\n",
    "\\vect{K_{**}} - \\vect{K_*}(\\vect{K}+\\sigma^2\\vect{I})^{-1})\\vect{K_*}^T)\n",
    "\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892d0e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:50:00.415082Z",
     "start_time": "2023-02-15T16:50:00.393271Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kernel_func = _partial(lib.KernelFuncs.squared_exponential, sigma=300, leng=3)\n",
    "anim = lib.Animate.gp_regression(\n",
    "    x, x_data, y_data, kernel_func, sigma_err=sigma_err, truth=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b757a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:50:51.897574Z",
     "start_time": "2023-02-15T16:50:51.788505Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5d715",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The posterior of the distribution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408d17b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:48:29.942906Z",
     "start_time": "2023-02-15T16:48:29.932947Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "post = lib.Regressions.non_parametric(\n",
    "    x, x_data, y_data, lib.KernelFuncs.squared_exponential, noise_var=sigma_err)\n",
    "anim = lib.Animate.gp_distribution(x, post, 'Linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f2496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T16:47:15.848510Z",
     "start_time": "2023-02-15T16:47:15.703679Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anim.repeat = False"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.850334,
   "position": {
    "height": "144.017px",
    "left": "750px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
